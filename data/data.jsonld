{
  "@context": {
    "schema": "https://schema.org/",
    "portfolio": "https://patrykgadziomski.github.io/lcs-portfolio.github.io#",
    "relatedTo": "schema:relatedLink",
    "hasPart": "schema:hasPart"
  },
  "@graph": [
    {
      "@id": "portfolio:intro",
      "@type": "schema:Article",
      "schema:name": "Intro",
      "schema:startDate": "2026-01-16",
      "schema:dateModified": "2026-01-26",
      "schema:authors": {
        "@id": "portfolio:patryk"
      },
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:lernportfolio-prüfungsformat"}
      ],
      "schema:content": "<h2>Willkommen auf meinem Lernportfolio!</h2><p>Dieses Portfolio dient der Dokumentation meiner Lernreise im Teilmodul <i>Datenformate</i> im Studiengang Bibliotheksinformatik an der Technischen Hochschule Wildau. Hier sammle ich Notizen und Erkenntnisse zu verschiedenen Themen aus dem Teilmodul.</p><br><p>Aber was ist ein Lernportfolio?<br><i>Das Lernportfolio bezeichnet eine Sammlung von Dokumenten (z. B. Arbeitsblätter, Protokolle, Essays) oder Materialien [...], die zu einem bestimmten Zeitpunkt unter Bezug auf ein inhaltlich umrissenes Lehrgebiet die Leistungen [...] dokumentieren und charakterisieren.</i><a href='https://www.fachportal-paedagogik.de/literatur/vollanzeige.html?FId=3208887' target='_blank' class='custom-link'>[1]</a></p><br><p>Das Besondere an diesem Lernportfolio ist die Integration eines Knowledge Graphs auf der rechten Seite. (Aus Gründen der Anzeige muss der Knowledge Graph die Grenzen des Fensters einhalten. Leider wird es dadurch etwas voll bzw. unübersichtlich, aber hoffentlich hält es sich in Grenzen.) Der Graph visualisiert die Struktur und Beziehungen zwischen den verschiedenen Notizen, ähnlich wie in Obsidian. Keine Werbung! Aber eine sehr tolle Software ;)</p><br><p><a href='https://obsidian.md/' target='_blank' class='custom-link'>→ Obsidian</a></p><br><p>Jeder Knoten repräsentiert eine Notiz, und die Verbindungen zeigen, wie die Themen miteinander zusammenhängen. Klicken Sie auf einen Knoten im Graph, um direkt zum entsprechenden Artikel zu springen!</p><br><p>Der Aufbau dieses Lernportfolios ist in zeitliche Abschnitte unterteilt, je nach den Terminen, an denen die Vorlesungen stattfanden. In diesen wird kurz zusammengefasst, was an diesen Tagen stattfand und was ich gelernt habe, in Bezug auf die Themen und weitere Erkenntnisse. Darauf folgen die zu bearbeitenden Pflichtaufgaben sowie ein abschließendes Fazit.<br> WICHTIG: Bei Pflichtaufgabe 2 sollte nur eine der drei Unteraufgaben bearbeitet werden. Aus Interesse am Thema wurden alle Unteraufgaben bearbeitet, jedoch wurde eine zur Bewertung für der Dozentin markiert!</p><br><p>Bei der technischen Umsetzung dieses Lernportfolios wurden Claude (Anthropic) und Euria (Infomaniak) eingesetzt, um die grundlegenden Funktionalitäten zu generieren. Weitere Funktionen, Strukturierung und Datenkuratierung wurden durch den Entwickler – mich :) – durchgeführt. Der Text in diesem Portfolio wurde ohne jegliche Hilfe von Sprachmodellen erstellt.</p><br><p>Folgende Fragen werden am Ende jeder Vorlesung als Zusammenfassung beantwortet und als zusammenfassendes Fazit:</p><br><ul style='padding-left: 15px;'><li>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</li><li>Was war Ihr größter Lerngewinn?</li><li>Was war besonders überraschend? Was hat Sie irritiert?</li><li>Wo sehen Sie für sich noch Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</li></ul><br><p>Aber ich will Sie nicht länger aufhalten. Stürzen Sie sich in die Welt der Daten und viel Spaß beim Erkunden meines Lernportfolios! :)</p>"
    },
    {
      "@id": "portfolio:vorlesung-1",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 1",
      "schema:startDate": "2025-10-15",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:references": [
        {"@id": "portfolio:datenformate-gbv"},
        {"@id": "portfolio:handbuch-it-in-bibliotheken"},
        {"@id": "portfolio:understanding-metadata"},
        {"@id": "portfolio:kleines-handbuch-metadaten"},
        {"@id": "portfolio:zugang-gestalten"},
        {"@id": "portfolio:date-meme"},
        {"@id": "portfolio:einführung-in-skos"},
        {"@id": "portfolio:marc-for-bibliographic"},
        {"@id": "portfolio:marc-dublin-core"},
        {"@id": "portfolio:marc-21"},
        {"@id": "portfolio:einführung-pica"}
      ],
      "schema:content": "<p>Dies war der dritte Tag des Studiums der Bibliotheksinformatik an der TH Wildau und zugleich der erste Tag des Moduls <i>Schnittstellen und Datenformate</i>. Das Modul ist in zwei Teile gegliedert: Der erste Teil, der von Tracy Arndt unterrichtet wird, behandelt Datenformate.</p><br><p>Die Vorlesung begann mit den üblichen organisatorischen Informationen und der Vorstellung. Anschließend wurden die Ziele des Teilmoduls sowie die einzelnen Blöcke vorgestellt. Der erste Block (Tag 1 und 2) befasst sich mit <i>Terminologie</i>, <i>Heterogenität</i>, <i>Interoperabilität</i> und <i>Organisation</i> von Metadaten. Natürlich wurden auch die Prüfungsleistungen und die dazugehörigen Bewertungskriterien erläutert. Das Lernportfolio als Prüfungsleistung finde ich persönlich sehr gut. Bereits im Bachelorstudium hatte ich häufig diese Form der Prüfungsleistung. Das Schöne daran ist für mich die Freiheit, die den Studierenden eingeräumt wird, ihr Portfolio auf individuelle Weise zu gestalten. Außerdem bin ich der festen Überzeugung, dass man nur durch Reflexion des Gelernten wirklich erkennt, ob man die Inhalte eines Moduls oder Fachs verstanden hat. Manchmal führe ich selbst ein Lerntagebuch für Themen, die ich mir selbst aneigne, da ich mir dabei die Frage stellen muss: Was habe ich eigentlich gelernt?</p><br><p>Zunächst klärten wir, was Daten überhaupt sind, also die <i>Terminologie</i>. Das war für mich nichts Neues, da ich im Bachelor einen Schwerpunkt auf Daten gelegt hatte. Auch Metadaten waren mir bereits bekannt (Metadatenmanagement wurde im Bachelorstudium als Wahlmodul belegt. Damals habe ich mich ausgiebig mit dem Datenformat <i>NeXus</i> im Rahmen einer Hausarbeit beschäftigt). Bibliothekarische Metadaten waren mir jedoch nur teilweise vertraut. Ich hatte zwar PICA und MARC im Studium kennengelernt, aber weder BibFrame noch Linked-Data-Formate. Linked Data finde ich äußerst interessant und möchte es überall dort einsetzen, wo es möglich ist, um es wirklich zu verstehen und zu erfahren, wie es funktioniert. Wie sich zeigt, bekomme ich dazu auch die Gelegenheit. Was das Thema <i>Heterogenität</i> angeht: Es war ein Thema, das mir erst in der Arbeitspraxis wirklich bewusst wurde. Im Studium wurde immer betont, dass man Standards einhalten muss, doch eine tatsächliche Rolle spielten sie erst, als man damit in Berührung kam.</p><br><p>Anschließend folgte eine Gruppenarbeit, in der wir uns mit verschiedenen Begriffen auseinandersetzen sollten. Dazu wurden uns Quellen zur Verfügung gestellt. Meine Gruppe und ich entwarfen daraufhin eine Visualisierung. Die Ergebnisse wurden anschließend präsentiert. Es wurde lebhaft darüber diskutiert, was genau ein Datenmodell ist und ob bzw. wie überall Standards auftreten können. Es war interessant zu sehen, dass selbst Menschen, die täglich mit dieser Thematik arbeiten, Schwierigkeiten haben, alles präzise zu beschreiben. Das war für mich zum Teil ein <i>Aha</i>- und ein <i>Wow</i>-Moment. Früher dachte ich immer, dass man, sobald man in einem Bereich arbeitet, diesen vollständig überblickt und versteht. Mittlerweile weiß ich es besser und dass es eine ständige Lernreise ist. In der Gruppe haben wir viel diskutiert und die Abbildung komplett neu gestaltet.</p><br><img src='imgs/datenformate.svg' style='width: 100%;'><figcaption><b>Abbildung 1</b>: Gruppenarbeitsergebnis im Teilmodul <i>Datenformate</i>, Visualisierung zur Beschreibung verschiedener Datenbegriffe und ihrer Zusammenhänge</figcaption><br><p>Spannend fand ich die <i>Interoperabilität</i> von Daten. Damit habe ich bisher nur wenige Berührungspunkte gehabt, was ich gerne ändern möchte. Ich fand die Konvertierung zwischen Formaten schon immer faszinierend. Ein Skript zu schreiben, das ein Format in ein anderes konvertiert, hat etwas beruhigendes. Dabei merke ich, dass ich statt vorhandener Software lieber eigene Skripte schreibe, was Vor- und Nachteile hat. Als auf der Folie <i>Kenntnis von Metadaten-Standards […]</i> auftauchte, erinnerte ich mich an ein Gespräch mit einer Bekannten über die Datenspeicherung physikalischer Vorgänge (Flugfahrttechnik) und ihr Problem, dass sie nicht wusste, wie sie solche Daten speichern und anschließend optimal ins System weitergeben sollte. Hier fehlte das Wissen über geeignete Datenformate, insbesondere über spezialisierte Formate für physikalische Vorgänge oder Experimente. Was man nicht kennt, kann man eben nicht nutzen.</p><br><p>Vor allem die Tatsache, dass es so viele Standards gibt und man sich auf keinen einigen kann (aus verschiedenen Gründen), fand ich sehr spannend. Im Modul <i>Künstliche Intelligenz</i> arbeite ich gerade daran, einen Linked-Data-Datensatz aufzubauen (dazu später mehr) und hätte beinahe einen eigenen Standard entwickelt, weil ich dachte: <i>Ah, wieso nicht? Dann kann ich es genau an meine Wünsche anpassen!</i> Am Ende habe ich mich jedoch für bestehende Standards entschieden, da sie alles enthielten, was ich benötigte, ich musste sie mir nur genauer ansehen.</p><br><p>In meinem Alltag kann ich durchaus behaupten, dass ich viel mit Daten zu tun habe. Das sollte eigentlich jeder behaupten können, da wir von Daten umgeben sind. Im beruflichen Kontext arbeite ich mit ISBD und MARC, da ich in meiner Bibliothek die Katalogisierung mit Koha durchführe. In meiner anderen Tätigkeit bin ich mit Webentwicklung beschäftigt, weshalb ich auch mit verschiedenen Datenformaten umgehen muss. Das alles war mir bereits aus dem Bachelorstudium bekannt. Dieser erste Tag war für mich somit eine nette Auffrischung, Neues habe ich nicht viel gelernt. Mein Hunger auf Linked Data ist jedoch gewachsen :)</p><br><p>Zusammenfassend kann man sagen:</p><br><ul style='padding-left: 15px;'><li><b>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</b><br>Inhaltlich habe ich mich mit den Grundlagen wie Terminologie, Heterogenität, Interoperabilität und Organisation von Daten beschäftigt, dabei habe ich mein Wissen zu Metadaten, PICA und MARC aufgefrischt und erstmals BibFrame sowie Linked Data kennengelernt. Persönlich hat mich überrascht, wie schwer es selbst erfahrenen Fachleuten fällt, Begriffe wie <i>Datenmodell</i> präzise zu beschreiben. Das hat mir gezeigt, dass Lernen nie wirklich fertig ist und dass es okay ist, nicht alles sofort zu verstehen. Ansonten habe ich bestätigt bekommen, dass Reflexion der Schlüssel zum Verständnis ist. Genau deshalb führe ich auch weiterhin mein Lerntagebuch, weil es mich zwingt, mir selbst zu fragen: <i>Was habe ich eigentlich gelernt?</i></li><li><b>Was war Ihr größter Lerngewinn?</b><br>Mein größter Lerngewinn liegt in der Erkenntnis, dass Standards nicht nur theoretisch, sondern praktisch relevant sind und dass es sinnvoll ist, bestehende Standards zu nutzen, statt eigene zu entwickeln, selbst wenn sie zunächst nicht perfekt erscheinen.</li><li><b>Was war besonders überraschend? Was hat Sie irritiert?</b><br>Überraschend fand ich, dass auch erfahrene Menschen Schwierigkeiten haben, grundlegende Begriffe wie <i>Datenmodell</i> präzise zu definieren. Das hat dich aus deiner Annahme gerissen, dass Expertise = vollständiges Verständnis bedeutet. Irritierend fand ich die Tatsache, dass es so viele Standards gibt und keine Einigung besteht.</li><li><b>Wo sehen Sie für sich noch Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</b><br>Ich möchte mich intensiver mit Interoperabilität und Linked Data beschäftigen, insbesondere mit praktischen Anwendungen und Konvertierungsprozessen. Ich plane, Linked Data im Modul <i>Künstliche Intelligenz</i> praktisch anzuwenden und dabei bestehende Standards zu nutzen, statt eigene zu entwickeln. Zudem möchte ich meine Skripting-Fähigkeiten weiter ausbauen, um Formate selbst zu konvertieren.</li></ul>"
    },
    {
      "@id": "portfolio:vorlesung-2",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 2",
      "schema:startDate": "2025-10-17",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-1"
      },
      "schema:references": [
        {"@id": "portfolio:einführung-pica"},
        {"@id": "portfolio:processing-marc-21"},
        {"@id": "portfolio:marc-must-die"},
        {"@id": "portfolio:marc-history-implications"},
        {"@id": "portfolio:handbuch-it-in-bibliotheken"},
        {"@id": "portfolio:library-reference-model"},
        {"@id": "portfolio:link-data-in-libraries"},
        {"@id": "portfolio:semantic-web"},
        {"@id": "portfolio:dalle-prompt"}
      ],
      "schema:content": "<p>An diesem Tag sind wir tiefer in die Materie der Metadaten eingetaucht und haben mit der Geschichte von <i>PICA</i> und <i>MARC</i> begonnen. Persönlich fand ich das Thema nicht besonders spannend, aber es war dennoch interessant zu sehen, wann diese Formate entstanden sind und wie sich die Anforderungen an Metadaten im Laufe der Zeit gewandelt haben. Ich merke, dass mein Interesse eher der Verwendung von Daten- und Metadatenformaten gilt, nicht ihrer Geschichte.</p><p>Das zentrale Thema des Tages war die <i>Metadatenorganisation</i>, ein hochspannendes, aber auch komplexes Feld. Allein die Tatsache, dass es Austauschformate braucht, weil es so viele unterschiedliche Metadatenformate gibt, macht das Ganze kompliziert. Ich habe mich gefragt, warum es nicht ein universelles Daten- oder Metadatenformat für alles geben kann. Doch das ist klar: Man kann nicht alles abbilden. Selbst wenn man ein solches Format ständig erweitern würde, würde es so viele Felder enthalten, dass es aus Speichergründen nicht mehr praktikabel wäre. Es geht vielmehr um aufgabenspezifische Nutzung. Die Vorstellung eines <i>Omni</i>-Formats ist zwar schön, aber unmöglich..</p><br><p>Was man hierbei wunderbar sieht: Metadaten gibt es überall. Egal, in welchem Bereich man arbeitet, man kommt immer mit ihnen in Berührung, auch wenn man es nicht bewusst wahrnimmt. Für mich wurde noch einmal deutlich, wo überall Metadaten vorkommen: in jedem Datenfluss, in jeder Datei, in jeder Kommunikation. Ich habe mir meine alten Projekte im Bachelorar noch einmal angesehen und bemerkt, wie oft Metadaten darin vorkamen und wie wenig ich damals darauf eingegangen bin. Jetzt hätte ich gerne die Zeit, alles noch einmal durchzugehen und zu überarbeiten.</p><br><p>Das FRBR-Modell war mir aus dem Bachelor bekannt, aber ich wusste nicht, dass es mehrere Varianten gibt oder weiterentwickelt wurde. Das FRBR-LRM hat mich überrascht. Da es ein sehr theoretisches Modell ist, entsteht für mich wieder das gleiche Problem: Es lässt sich nicht alles abbilden, und man kann sich ewig darüber streiten, wie man es anwendet. Beim Thema ETL musste ich zugeben, dass mir das bereits bekannt war. Im Bachelor hatte ich viele Module aus dem Bereich Data Science belegt.</p><br><p>Dann kam das unglaublich spannende Thema: <i>Linked (Open) Data</i>. Im Studium hatte ich bereits damit zu tun, da sich mein Professor damit beschäftigt, aber ich konnte mir darunter nie etwas vorstellen. Für mich war es eine rein theoretische Vorstellung: ein Web, in dem Daten als Knoten definiert sind, die Verbindungen zu anderen Knoten und somit zu anderen Daten herstellen. Theoretisch verstanden, aber praktisch? Keine Ahnung. Bis jetzt. Jetzt habe ich es verstanden: Es ist ein <i>Knowledge Graph</i>! Ein Knowledge Graph entsteht durch das Verbinden von Daten (Nodes) und das ist Linked Data. Die Idee, dass das Web im Web-3.0-Modell komplett aus aufeinander verweisenden Daten besteht, finde ich unglaublich schön. Allein die Möglichkeit, semantische Anfragen zu stellen, um genau die gesuchten Daten zu finden, ist für die Suche und Suchmaschinen enorm wichtig. Leider wurde diese Entwicklung durch das Aufkommen großer Sprachmodelle gebremst, oder zumindest in den Hintergrund gedrängt. Ich stelle mir ein perfektes Modell vielmehr als eine Verbindung von beidem vor: Das Semantic Web könnte eine unglaubliche Datenquelle sein, auf die KI-Agenten zugreifen könnten. Ich habe dazu einmal eine Visualisierung gesehen, finde sie aber nicht mehr. Also habe ich versucht, sie mit ChatGPTs DALL-E zu generieren.</p><br><img src='imgs/web3andAI.png' style='width: 100%;'><figcaption><b>Abbildung 2</b>: Vorstellung eines Web-Models, bei welchem Semantic Web und KI-Agenten harmonisch koexistieren - erstellt mit ChatGPTs DALL-E</figcaption><br><p>Jetzt: Warum glaube ich, dass ich Linked Data bereits seit Jahren nutze?</p><br><p>Ich benutze seit Jahren Obsidian, um meine Notizen zu verwalten. Die Software erlaubt es, Verbindungen zwischen Notizen herzustellen und damit war mir das Prinzip, Informationen zu verknüpfen, bereits sehr vertraut. Aber jetzt habe ich es vollständig verstanden: Die einzelnen Knoten in meinem Kopf zum Thema Linked Data haben sich endlich verbunden :) Ich habe meine Notizen die ganze Zeit wie Linked Data behandelt, ohne es zu wissen. Seitdem nutze ich Standards wie Schema.org, Dublin Core und Friend of a Friend, um meine Daten noch besser zu strukturieren und zu verknüpfen. Jetzt kann ich meine ganzen Notizen wirklich als <i>Linekd Data</i> bezeichnen. Da sind aber zum Teil privat sind, nicht als <i>Linked Open Data</i>. Ich habe aber tatsächlich schon darüber nachgedacht meine ganze Aufschriebe und Lerntagebücher, genau wie dieses per GitHub Repository als Linked Open Data zu veröffentlichen. Die Frage bleibt nur ob sich das Lohnt, und ob man das nicht evtl. anders tun sollte. Ich bin für Vorschläge offen :) Warum? Im Modul <i>Künstliche Intelligenz</i> arbeite ich mit einer Kommilitonin an einem Projekt zum Thema Retrieval Augmented Generation (RAG). Dafür erstelle ich, wie bereits erwähnt, einen Linked-Data-Datensatz. Ziel ist es, am Ende ein RAG-System zu haben, mit dem man im Obsidian-Vault suchen oder Fragen stellen kann. Dabei soll das System nicht nur die Ähnlichkeit zwischen Anfrage und Dokumenten in der Vektor-Datenbank berücksichtigen, sondern auch die Dokumente, die mit diesen verknüpft sind. Sehr interessant war auch die Erkenntnis, dass Linked Data besser für das Training oder die Nutzung von KI-Modellen wie LLMs geeignet ist. Das passt perfekt zu unserem Projekt. Wenn es funktioniert, werde ich es auch für meinen privaten Vault nutzen und kann dann quasi <i>mit mir selbst reden</i>, da er viele Informationen aus meinem Leben enthält. (Der Datenschutz muss natürlich beachtet werden. Ich möchte nicht, dass meine Daten in irgendein LLM gelangen.)</p><br><p>Zusammenfassend kann man sagen:</p><br><ul style='padding-left: 15px;'><li><b>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</b><br>Inhaltlich habe ich mich mit der Metadatenorganisation auseinandergesetzt, insbesondere mit der Notwendigkeit von Austauschformaten und der Begrenztheit universeller Formate. Ich habe das FRBR-LRM vertieft und Linked Data endlich verstanden, nicht nur theoretisch, sondern auch praktisch im Kontext von Knowledge Graphs und Obsidian. Persönlich habe ich erkannt, dass ich bereits seit Jahren mit Linked Data arbeitest, ohne es zu wissen und dass meine eigene Arbeitsweise (Obsidian, Verknüpfungen) perfekt dazu passt. Ansonsten habe ich gesehen, wie Metadaten überall vorkommen, auch in meinen eigenen Arbeiten und ich will sie nun bewusster nutzen. Ich habe auch erkannt, dass KI und Semantic Web gut zusammenpassen und will das in meinem Projekt nutzen.</li><li><b>Was war Ihr größter Lerngewinn?</b><br>Mein größter Lerngewinn ist die Erkenntnis, dass ich Linked Data bereits in deiner täglichen Praxis anwende und dass ich es jetzt verstanden habe. Das verbindet Theorie und Praxis und zeigt mir, dass ich nicht nur lerne, sondern auch bereits wirksam handle.</li><li><b>Was war besonders überraschend? Was hat Sie irritiert?</b><br>Überraschend fand ich, dass ich Linked Data bereits seit Jahren nutze und dass es nicht nur Theorie ist, sondern praktisch anwendbar. Auch das FRBR-LRM hat mich überrascht, weil es weiterentwickelt wurde.Irritiert hat mich die Ttsache, dass große Sprachmodelle die Entwicklung des Semantic Web gebremst haben.</li><li><b>Wo sehen Sie für sich noch Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</b><br>Ich möchte tiefer in Linked Data einsteigen, insbesondere in Praxisanwendungen, Standards (Schema.org, FOAF, Dublin Core) und Integration mit KI-Systemen wie RAG. Ich will außerdem mein RAG-Projekt mit Linked Data erfolgreich abschließen und es dann für meinen privaten Obsidian-Vault nutzen, um <i>mit mir selbst zu reden</i>.</li></ul>"
    },
    {
      "@id": "portfolio:vorlesung-3",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 3",
      "schema:startDate": "2025-12-03",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-2"
      },
      "schema:references": [
        {"@id": "portfolio:semantic-web"},
        {"@id": "portfolio:w3c-standards-and-drafts"},
        {"@id": "portfolio:semantic-web-working-ontologist"},
        {"@id": "portfolio:validating-rdf-data"}
      ],
      "schema:content": "<p>Das Thema <i>Linked Data</i> und <i>Semantic Web</i> wurde an diesem Tag weiter vertieft und man kann durchaus sagen: Meine Lernkurve in diesem Bereich war exponentiell!</p><br><p>An diesem Tag habe ich endlich verstanden, wie man <i>Ontologien</i> aufbaut und <i>RDF-Standards</i> verwendet. Zwar hatte ich damit bereits experimentiert, aber erst durch die Übung in der Vorlesung wurde alles viel klarer. Bisher dachte ich immer, Ontologien seien komplexe, textuelle Bäume, was so gesehen stimmt, aber auch wieder nicht.</p><br><p>PS: Vor dem Masterstudium habe ich mir Jobs im Bereich Ontologien angesehen und dachte, sie klingen zwar interessant, aber ich würde es bestimmt nicht hinbekommen, weil mir das Wissen fehlte. Jetzt weiß ich: Mir hat das Wissen gefehlt, aber die Praxis war da. Ich wusste nur nicht, dass das, was ich in Obsidian erstellt habe, mehr oder weniger Ontologien waren.</p><br><p>Ontologien lassen sich als das darstellen, was ich sowieso fast jeden Tag mache, oder zumindest für mein privates Leben oft tue: Ich erstelle Beschreibungen von Objekten und deren Beziehungen zueinander, in Form eines Knowledge Graphs. Das war wie eine Erleuchtung für mich. Anhand dieser Ontologien kann man Objekte beschreiben und dafür gibt es verschiedene Standards. Das war ein großer Lerneffekt und ein noch größerer Aha-Moment. Vor allem durch das Üben mit <i>WebVOWL</i> wurde mir klar, wie Ontologien visuell aussehen können.</p><br><p>Zwei Übungen haben mir unglaublich viel gebracht: <a href='data/PG_Ontology.ttl' target='_blank'>Eigene Ontologie</a> und <a href='data/bim25graph.ttl' target='_blank'>BIM25 Ontologie</a>.</p><br><p>Neben diesem Thema wurde SPARQL kurz angesprochen, das wird aber am nächsten Tag genauer behandelt. Das RDF-Schema als Tripel kannte ich bereits, es war somit leicht verständlich, und ich konnte schnell in die Übungen einsteigen. Was ich nicht kannte, waren die Standards wie FOAF, sowie die RDF-Serialisierung. Doch alles war sehr verständlich.</p><br><p>Für mein Lernportfolio habe ich mich für <i>JSON-LD</i> als Datenformat für Linked Data entschieden. Der Grund: die einfache Lesbarkeit, da es auf der Syntax von JSON basiert und ein modernes Format ist. Am Anfang finden Definitionen im Context statt, unter anderem auch von meinem eigenen Standard, da ich für das Portfolio teilweise sehr spezifische Beziehungen brauchte, oder es einfach ausprobieren wollte, wie es mit einem eigenen funktioniert. Ich hätte natürlich Standards wie Schema.org verwenden können, aber der Lerneffekt war so größer. Der weitere Aufbau des Datensatzes erfolgte wie bei einer normalen JSON-Datei, mit der Ausnahme, dass die Keys die Syntax <i>standard:relation</i> verwenden. Die Erstellung dieses Datensatzes sah ich als sehr wichtige Übung an um mit einem echten Linked-Data-Format zu arbeiten und vor allem, um die Visualisierung als Knowledge Graph zu üben.</p><br><p>SSehr schöne Ressourcen, die genannt wurden, möchte ich hier noch einmal festhalten:: <ul style='padding-left: 15px;'><li><a href='https://www.ldf.fi/service/rdf-grapher' target='_blank'>RDF Grapher</a></li><li><a href='https://issemantic.net/rdf-visualizer' target='_blank'>Visualize RDF graph linked data as a connected diagram</a></li><li><a href='https://issemantic.net/rdf-converter' target='_blank'>Online RDF converter and validator to JSON-LD, Microdata, Turtle, TriG, RDF-star or any other serialization format</a></li><li><a href='http://ttl.summerofcode.be/' target='_blank'>IDLab Turtle Validator</a></li></ul></p<br><br><p>Und jetzt noch einmal zum Thema Jobsuche: Jetzt, da ich endlich verstanden habe, was Linked Data und Ontologien sind, habe ich mir erneut Jobs in diesem Bereich angesehen. Hier eine kleine Sammlung – damit ich später noch weiß, was in diesem Bereich möglich ist. (Die Links könnten mittlerweile ungültig sein, da die Stellenanzeigen abgelaufen sein könnten.)</p><br><ul style='padding-left: 15px;'><li><b>Wissenschaftliche:r Mitarbeiter:in (w/d/m) Schwerpunkt Wissensgraph</b> - <a href='https://karriere.preussischer-kulturbesitz.de/jobposting/e13514c77a524fa3a13a6f4d3c4b2ab2b69c8f710' target='_blank' class='custom-link'>Link</a> - Dabei geht es um den Aufbau und Pflege eines Wissensgraphen sowie der Datenmodellierung und Ontologieintegration. Klingt mittlerweile nicht mehr so abstrakt für mich :)</li><li><b>Wissenschaftliche:r Mitarbeiter:in (w/d/m) Schwerpunkt ETL-Prozesse & Ontologie-Entwicklung</b> - <a href='https://karriere.preussischer-kulturbesitz.de/jobposting/4443429e02664d89ef4df65a1d610bcd884c7abe0' target='_blank' class='custom-link'>Link</a> - Hier lese ich viel über Transformationsmodellen mit Datenformate, die mir leider noch nicht ganz bekannt vorkommen, aber auch der Erarbeitung von Ontologien!</li><li><b>Knowledge Engineer (m/w/d)</b> - <a href='https://de.indeed.com/viewjob?jk=245f60421081e29d&tk=1jf98dn8chn1h85b&from=serp&vjs=3' target='_blank' class='custom-link'>Link</a> - Diese Stelle, auch wenn Worte wie Ontologien, Metadaten und Wissensgraphen gefallen sind, ist viel wirtschaftlicher und vielvältiger aufgestellt. Hier muss man auch Data Science Erfahrung haben</li><li><b>Senior Developer (f/m/d) for Knowledge Graph Research</b> - <a href='https://jobtensor.com/job/Senior-Developer-fmd-for-Knowledge-Graph-Research-8518c0254b1d' target='_blank' class='custom-link'>Link</a> - Eine sehr interessante Stelle, mit einer riesigen akademischen Verantwortung. Nichts für mich (noch nicht).</li><li><b>Knowledge and Ontology Engineer - Insurance (m/f/d)*</b> - <a href='https://jobtensor.com/job/Knowledge-and-Ontology-Engineer-Insurance-mfd-340bfa3ff9d6' target='_blank' class='custom-link'>Link</a> - Wieder eine sehr wirtschaftliche Stelle, die auch wieder sehr im Bereich von Data Science liegt.</b></i></ul><br><p>Was mir bei der Recherche aufgefallen ist: Viele Stellen sind sehr verwandt mit denen von Data Engineer oder Data Scientist. Mir ist klar, dass man Ontologien und Knowledge Graphs auch in diesen Bereichen einsetzt – aber man muss stark darauf achten, zwischen bibliothekarischen und wirtschaftlichen Stellen zu unterscheiden. Ich fand es schön, die Stellenanzeigen jetzt lesen zu können – ohne das damalige Gefühl der Überforderung. Jetzt habe ich das Wissen aus dem Teilmodul – und muss es nur noch in der Praxis umsetzen und meine Fähigkeiten in diesem Bereich weiter ausbauen.</p><br><p>Zusammenfassend kann man sagen:</p><br><ul style='padding-left: 15px;'><li><b>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</b><br>Inhaltlich habe ich verstanden, wie Ontologien aufgebaut und RDF-Standards verwendet werden, insbesondere mit WebVOWL und JSON-LD. Ich hbe auch SPARQL und RDF-Serialisierungen kennengelernt und konnte mein Wissen in Übungen anwenden. Persönlich habe ich erkannt, dass ich bereits praktische Erfahrung mit Ontologien habe, durch ,eine Arbeit mit Obsidian. Das hat mein Selbstvertrauen gestärkt. Ansonsten habe ich Ressourcen gesammelt (RDF Grapher, Turtle Validator etc.) und Jobs im Bereich Ontologien und Knowledge Graphs recherchiert und dabei festgestellt, dass ich jetzt die Stellenanzeigen ohne Angst lesen kannst.</li><li><b>Was war Ihr größter Lerngewinn?</b><br>Mein größter Lerngewinn ist die Erkenntnis, dass ich bereits Ontologien erstellt und dass ich sie jetzt verstanden habe. Außerdem war die Erstellung meines JSON-LD-Datensatzes eine praktische Übung, die mir gezeigt hat, wie man mit Linked Data arbeitet und wie man es visualisiert.</li><li><b>Was war besonders überraschend? Was hat Sie irritiert?</b><br>Ich war überrascht dass ich bereits Ontologien erstellt habe, ohne es zu wissen. Auch die Vielfalt der Jobs im Bereich Knowledge Graphs und Ontologien hat mich überrascht. Irritierend fand ich, dass viele Stellen sehr nah an Data Science liegen und dass man zwischen bibliothekarischen und wirtschaftlichen Stellen unterscheiden muss.</li><li><b>Wo sehen Sie für sich noch Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</b><br>Ich möchte tiefer in SPARQL einsteigen und weitere RDF-Serialisierungen wie Turtle, TriG oder RDF-star kennenlernen. Ich will auch mehr mit WebVOWL und anderen Visualisierungstools arbeiten und weitere Standards wie SKOS oder OWL erforschen. Ich willst mein JSON-LD-Projekt weiter ausbauen (mein Portfolio fertigstellen) und es in deinem RAG-System nutzen. Ich will auch mehr praktische Erfahrung mit Ontologien sammeln und vielleicht sogar eine eigene Ontologie für deinen Vault erstellen. Langfristig: Ich will mich evtl. auf Jobs im Bereich Knowledge Graphs bewerben, aber erst, wenn ich mich sicher fühle.</li></ul>"
    },
    {
      "@id": "portfolio:vorlesung-4",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 4",
      "schema:startDate": "2025-12-04",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-3"
      },
      "schema:references": [
        {"@id": "portfolio:sparql-overview"},
        {"@id": "portfolio:wikidata-sparql-tutorial"},
        {"@id": "portfolio:wikidata-sparql-query-service"},
        {"@id": "portfolio:sparql-by-example"},
        {"@id": "portfolio:wikibooks-sparql"},
        {"@id": "portfolio:sparql-tutorial"},
        {"@id": "portfolio:learning-sparql"}
      ],
      "schema:content": "<p>An diesem Tag wurde <i>SPARQL</i> vorgestellt. Ich muss zugeben, dass es der Tag war, an dem ich am wenigsten mitgenommen habe. Nicht, weil das Thema nicht spannend ist. Ich finde es unglaublich, was man mit SPARQL alles abfragen kann. Aber ich bin einfach nicht für Online-Vorlesungen gemacht. In einer normalen Vorlesung höre ich zu, achte auf und stelle gelegentlich Fragen. Dabei kann ich mich komplett darauf einstellen. Bei einer Online-Vorlesung funktioniert das nicht, meine Konzentration schweift komplett ab. Aus diesem Grund fiel es mir unglaublich schwer, der Vorlesung zu folgen, aber ein paar Dinge konnte ich doch mitnehmen.</p><br><p>Mit SPARQL hatte ich bisher nur einen Berührungspunkt: Im Bachelor-Modul <i>Metadatenmanagement</i> habe ich es kennengelernt, aber ich muss gestehen, dass ich es bis heute nicht ganz durchblicke. Mir ist durchaus verständlich, wie SPARQL funktioniert, aber die Syntax ist mein größtes Problem. Ich müsste mir theoretisch alle Codes für die Entitäten merken, wenn es keine Liste gibt bzw. ich keine zur Hand habe. Das ist mein Problem. Das ist sehr ungewohnt. Wenn ich Python-Code oder eine SQL-Abfrage schreibe, muss ich normalerweise nichts nachschauen, außer wie die Spalten heißen. Ich weiß allerdings nicht, ob man das so vergleichen kann. Aus diesem Grund und aufgrund meiner mangelnden Online-Konzentration hatte ich Schwierigkeiten, den Übungsaufgaben zu folgen. Was mir tatsächlich leichtfiel, waren die Funktionen in SPARQL wie Filtern oder Gruppieren, da diese sehr ähnlich zu SQL sind. Ich habe mir aber vorgenommen, das nachzuholen, denn ich sehe ein riesiges Potenzial für Information Retrieval und eventuell auch für meine privaten Projekte, vor allem, da man auch geografische Daten abfragen kann. Hierbei finde ich auch die Visualisierungen der Ergebnisse der Abfragen sehr hilfreich. Nicht nur lassen sich einfache Graphen wie Balkendiagramme ausgeben, sondern ebenfalls Karten und Timelines. Im Modul <i>Management and Leadership</i>, in welchem wir ein Projekt mit der Apple Vision Pro durchführen, haben wir uns vorgenommen aus SPARQL zu nutzen. Das Ziel der Teilgruppe in der ich Mitglied bin, ist es zu schauen, wie man Medien in Bibliotheken, oder anderen Kultureinrichtungen, während man die Apple Vision Pro verwendet, anreichern kann. Da wir Informationen über ein bestimmtes Objekt bekommen müssen, bietet sich SPARQL besonders gut an.</p><br><p>Persönlich habe ich mir vorgenommen, das Buch <i>Learning SPARQL: Querying and Updating with SPARQL</i> von O’Reilly Media durchzuarbeiten, um am Ende behaupten zu können, SPARQL wirklich verstanden zu haben. Da ich aber derzeit kein Projekt habe, in dem ich SPARQL verwenden muss, und auch nicht die Zeit habe, das Buch durchzuarbeiten, wird sich das Thema SPARQL für mich leider noch etwas nach hinten verschieben. Ein anderes Thema für mich war noch Wikidata.org, welches mir ebenfalls schon bekannt war. Ich habe in Vergangenheit bei Wikipedia Workshops teilgenommen und fand diese ganze Initiative immer schon besonders spannend. Bei Wikipedia wurde ich jedoch nie ganz warm. Wikidata finde ich persönlich ein unglaubliches Werk, wo man sehr viele Informationen über alle möglichen Dinge erfährt und abfragen kann. SPARQL ist dabei ein Muss. Ein Grund mehr SPARQL zu lernen."
    },
    {
      "@id": "portfolio:vorlesung-5",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 5",
      "schema:startDate": "2026-01-16",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-4"
      },
      "schema:references": [
        {"@id": "portfolio:openrefine-user-manual"},
        {"@id": "portfolio:metafacture-tutorial"}
      ],
      "schema:content": "<p>Unverzichtbar sind Werkzeuge, mit denen sich Daten ansehen, bearbeiten, analysieren und verarbeiten lassen. Dabei wurden unterschiedliche Softwares gezeigt und zwei davon haben wir in einem kleinen Workshop selbst ausprobiert.</p><br><p>Am letzten Tag des Teilmoduls fand ein Workshop zu <i>OpenRefine</i> und <i>Metafacture</i> statt. Bei Open Refine wurden verschiedene Funktionen anhand der Dokumentation gezeigt und ausprobiert. Bei Metafacture wurden die Funktionen anhand eines Tutorials gezeigt. Persönlich nutze ich solche Software in meiner Praxis nicht wirklich. Wenn ich Operationen zum Filtern oder Clustering großer Datenmengen durchführe, dann mache ich das meistens in Python mit Bibliotheken, die mit großen Datenmengen arbeiten können. Da ich das oft tue, bin ich mit Python bei der Datenanalyse auch schneller als mit Software mit einer GUI. Ich kann aber nachvollziehen, dass eine GUI-Software für Personen, die nicht oft programmieren, von Vorteil ist. Sehr praktisch fand ich den Abgleich der Daten mit anderen Datensätzen (z. B. dem Datensatz der DNB).</p><br><p>Bei Metafacture war mein Eindruck ähnlich. Ich fand das Einlesen eines Datensatzes (z. B. in JSON) und das Exportieren in einem anderen Datenformat aber sehr praktisch. Ansonsten konnte ich für meine alltägliche Arbeit keinen großen Nutzen daraus ziehen. Das kann sich aber noch ändern.</p><br><p>Am Ende folgte eine kleine Diskussion, die mehr oder weniger genau in diese Richtung ging. Einerseits hat man das Potenzial der Software gesehen, aber da sie nur im Nischenbereich von sehr wenigen Leuten eingesetzt wird, ist es fragwürdig, wieso man sie überhaupt lernen sollte. Vor allem, da man sich erst mit der Software auseinandersetzen müsste, da sie nicht ganz unkomplex ist. Ich glaube, die in der Gruppe, die täglich programmieren, sind es eher gewohnt, solche Operationen mit Python und Pandas durchzuführen (solange der Speicher es erlaubt), während die anderen das Problem haben, dass sie in ihrer alltäglichen Arbeit einfach keine Verwendung für solche Daten-Software finden, da sie nicht täglich mit Daten arbeiten."
    },
    {
      "@id": "portfolio:aufgabe-1",
      "@type": "schema:Assignment",
      "schema:name": "Aufgabe 1 - Datenworkflow",
      "schema:courseCode": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:workflow-digital-sammlungen"}
      ],
      "schema:content": "<h3>Die Staatsbibliothek zu Berlin hat auf ihrer Webseite den <a href='https://digital.staatsbibliothek-berlin.de/ueber-digitalisierte-sammlungen/digiworkflow' target='_blank'>Workflow für die Digitalisierung</a> veröffentlicht.<br>Erstellen sie aus der Liste ein Diagramm das den Workflow grafisch darstellt. Reflektieren sie anschließend den Workflow und beschreiben sie die eingesetzten Datenformate und Schnittstellen (TIFF, METS/MODS, IIIF-Manifest, OAI-PMH)</h3><br><img src='imgs/digitalisierte_sammlungen.svg' style='width: 100%;'><br><p>Was mir auffällt: Es sind sehr viele Schritte und Akteure beteiligt. Man denkt bei solchen Prozessen, dass sie eigentlich ganz einfach sein könnten. Das Werk wird angeschaut, gescannt, katalogisiert und fertig. Die Vorstellung einer Digitalisierung ist viel simpler, als sie in der realen Praxis aussieht. Einen Grund für eine selektive Digitalisierung kann ich vollständig nachvollziehen: Nicht jedes Werk soll oder muss digitalisiert werden (was aus meiner Sicht eine sehr gute Sache wäre, aber aufgrund des Aufwands, den ich durch den Workflow kennengelernt habe, nicht möglich ist). Bereitstellung, Speicherung und Katalogisierung erscheinen mir sinnvoll. Die Übernahme bibliografischer Daten ist selbstverständlich ein Muss. Das Anlegen eines Vorgangs zur Bearbeitung macht zwar Sinn, klingt aber nach bürokratischer Mehrarbeit, je nachdem, wie umständlich das System ist. Ich kann mir das aktuell nicht vorstellen, da ich keinen Einblick in das Workflow-Management-Tool Kitodo habe. (Ich habe einen Blick auf Kitodo geworfen und verstehe jetzt, dass es kein Weg herum um die Softwre gibt. Es ist viel mehr ein notwendiges Tool, als bürokratische Mehrarbeit, was ich gedacht habe.)</p><br><p>Ein Punkt, der mich persönlich überrascht hat, aber schnell nachvollziehbar wurde: die Lieferung an das Digitalisierungszentrum. Aus irgendeinem Grund dachte ich, dass die Digitalisierung vor Ort in der Bibliothek stattfindet. Dass ein dritter Akteur hier mitwirkt, macht unglaublich viel Sinn, denn die Bibliothek verfügt in der Regel weder über die Kapazität noch über das Personal für eine solche Aufgabe, ganz zu schweigen von den technischen Geräten und Maschinen.</p><br><p>Alle weiteren Schritte, Entscheidung über Digitalisierbarkeit, Entscheidung über Restaurierung, Wahl des Digitalisierungsverfahrens sowie Aufnahme und Speicherung, waren mir bekannt, da sie im Bachelorstudium im Modul Cultural Heritage behandelt wurden. Dabei wurde jedoch der praktische Teil rund um das Digitalisieren fokussiert, nicht die Abläufe davor und danach. Eine Qualitätsprüfung ist durchaus sinnvoll. Über Paginierung und Strukturerfassung hätte ich nicht nachgedacht, es macht selbstverständlich Sinn, ich dachte aber, dass diese Daten bereits vorliegen, da sie bei der Katalogisierung erfasst worden sein müssten.</p><br><p>Der Block mit Endkontrolle, Rücktransport und Freigabe ist wiederum sehr verständlich. Ich finde persönlich sehr interessant, dass die Qualitätskontrolle der Metadaten automatisiert stattfindet. Ich würde gerne erfahren, wie genau sie automatisiert wurde und auf welche Kriterien geachtet wird. (Aus Neugier habe ich daraufhin mir METS und MODS Validatoren angeschaut um zu verstehen, wie eine Qualitötskontrolle aussehen könnte.)</p><br><p>An dieser Stelle kommt die Archivierung des Master-TIFFs, und ich hatte zunächst keine Ahnung, was ein TIFF ist.</p><br><p><b>TIFF</b>: <i>Das TIFF-Format ist ein beliebtes Dateiformat in Grafikdesign und Fotografie. TIFF ist eine Abkürzung für Tagged Image File Format. Es kann Rastergrafiken und Bildinformationen speichern und zwar hochwertig, ohne Qualitätsverlust. Das Format wird von Windows, Linux und macOS unterstützt. Im Vergleich zu anderen Bilddateiformaten ist TIFF größer und verbraucht mehr Speicherplatz, kann aber zusätzliche Bildinformationen und -daten speichern. Interessant: TIFF-Dateien können auch als Container für kleinere JPEG-Dateien fungieren.</i> [1]</p><br><p><b>Master-TIFF</b>: <i>Die Master-TIFF ist eine TIFF-Datei, die als primäres Arbeits- oder Archivierungsdatei dient.</i> [1]</p><br><p>Nun, da mir bewusst wurde, was das TIFF-Format ist, komme ich zum OCR (Optical Character Recognition). OCR ist mir klar und ein Begriff. Es geht um die Texterkennung des Mediums und dessen Digitalisierung, anstatt einfach nur Kopien der Seiten anzufertigen.</p><br><p>Ein Block, der mir zunächst wenig verständlich vorkam, da ich vieles nicht kannte: die Erzeugung der METS/MODS-Dateien. Worum handelt es sich dabei?</p><br><p><b>METS/MODS</b>: <i>METS (Metadata Encoding and Transmission Format) und MODS (Metadata Object Description Schema) sind Metadatenformate für den Austausch von Daten zu digitalisierten Drucken. [2] Beide Formate sind mit XML-Schema definierte XML-Formate. [3][4] METS unterstützt einerseits die Verwaltung der digitalen Objekte innerhalb einer Datenbank und dient andererseits auch als Austauschformat. Dabei dient METS als Container, in dem neben den administrativen Eigenschaften der Texte vor allem auch die strukturellen Eigenschaften beschrieben und die verschiedenen Teile eines digitalisierten Textes zusammengeführt werden. Für die bibliografische Beschreibung dieser Teile wird nicht der METS-Standard verwendet, vielmehr verweist METS in diesem Zusammenhang auf andere Metadatenstandards wie z. B. MODS. [3] Ziel von MODS war, für kleinere Bibliotheksanwendungen einen Standard bereitzustellen, der reichhaltiger ist als Dublin Core, aber einfacher als MARC. Dementsprechend enthält der Standard einige Entsprechungen zu MARC-Feldern, vereinfacht diese jedoch bzw. gruppiert sie um.</i> [5]</p><br><p>Nun ist auch dies klar und mein Interesse wurde geweckt. Ich würde gerne sehen, wie solche digitalisierten Werke mit ihren METS/MODS-Dateien aussehen.</p><br><p>Die Verbindung der Bilddateien mit Volltexten ist klar, genau wie die Bereitstellung der Ausgangsbilder für die Präsentation. Die Einführung in den Suchindex ist mir bekannt, da ich bereits Erfahrung in der Indexierung bei Suchmaschinen habe. Die Aggregation zusammengehöriger METS/MODS-Dateien macht in diesem Kontext nun auch für mich Sinn.</p><br><p>Ein weiterer Teil des Workflows, der mir zunächst ein Rätsel war: das Erstellen der IIIF-Manifeste.</p><br><p><b>IIIF-Manifest</b>: <i>Unter IIIF (International Image Interoperability Framework) versteht man ein Rahmenmodell, das beschreibt, wie hochaufgelöste Digitalisate mit einheitlichen Metadaten frei zugänglich gemacht werden können. IIIF besteht aus verschiedenen APIs:</i></p><br><ul style='padding-left: 15px;'><li><i>IIIF Image API: ein Webservice zur Ausgabe von Digitalisaten</i></li><li><i>IIIF Presentation API: beschreibt die Digitalisate eines Objektes mit ihren bibliografischen und strukturellen Metadaten</i></li><li><i>sowie: IIIF Content Search API, IIIF Authentication API, IIIF Change Discovery API, IIIF Content State API, die hier nicht weiter erläutert werden.</i>[6]</li></ul><br><p>Über so viele Formate auf einmal habe ich noch nie gelernt. Sas ist zwar viel, aber der Digitalisierungsworkflow wird dadurch immer überschaubarer und verständlicher. Die Prozesse sehe ich fast schon vor meinem inneren Auge.</p><br><p>Die Bereitstellung des Werkes in den digitalisierten Sammlungen ist wieder ein klarer Schritt. Dann kommt die Bereitstellung als DFG-Viewer, IIIF und OAI-PMH oder als PDF-Download. Was IIIF und PDF sind, ist mir klar. Der DFG-Viewer ist eine Referenzimplementierung für die Digitalisierungsstandards der DFG (Deutsche Forschungsgemeinschaft). [7]</p><br><p>Etwas sehr Neues hierbei ist das OAI-PMH.</p><br><p><b>OAI-PMH</b>:  <i>OAI-PMH steht für OAI Protocol for Metadata Harvesting und wird zur Weitergabe von Metadaten an Portale oder Kataloge genutzt. Es kann auch zur Übertragung anderer Daten, z. B. Angaben zu Zitationen, eingesetzt werden und wird von einer wachsenden Anzahl von Institutionen unterstützt, z. B. auch vom Internet Archive. [8] OAI steht in diesem Zusammenhang für Open Archives Initiative.</i>[9]</p><br><p>Nachdem das geklärt und verstanden wurde, geht es weiter mit einer nachträglichen und wiederholten Volltexterkennung, falls gewünscht. Das Thema OCR hatten wir bereits angesprochen, also geht es weiter mit der Bereitstellung des Werkes für die Übernahme. Damit endet auch der Workflow und ein neues Werk kann für eine Digitalisierung vorgeschlagen werden.</p><br><p><ul style='list-style-type: none;'><li>[1] Adobe: TIFF-Dateiformat. <a href='https://www.adobe.com/de/creativecloud/file-types/image/raster/tiff-file.html' target='_blank'>https://www.adobe.com/de/creativecloud/file-types/image/raster/tiff-file.html</a></li><li>[2] Deutsche Digitale Bibliothek: METS/MODS. <a href='https://pro.deutsche-digitale-bibliothek.de/glossar/metsmods-format' target='_blank'>https://pro.deutsche-digitale-bibliothek.de/glossar/metsmods-format</a></li><li>[3] Wikipedia: METS. <a href='https://de.wikipedia.org/wiki/Metadata_Encoding_%26_Transmission_Standard' target='_blank'>https://de.wikipedia.org/wiki/Metadata_Encoding_%26_Transmission_Standard</a></li><li>[4] Wikipedia: MODS. <a href='https://de.wikipedia.org/wiki/Metadata_Object_Description_Schema' target='_blank'>https://de.wikipedia.org/wiki/Metadata_Object_Description_Schema</a></li><li>[5] Deutsche Digitale Bibliothek: Einführung in METS/MODS. <a href='https://wiki.deutsche-digitale-bibliothek.de/spaces/DFD/pages/69124677/1.+Einf%C3%BChrung+in+METS+MODS' target='_blank'>https://wiki.deutsche-digitale-bibliothek.de/spaces/DFD/pages/69124677/1.+Einf%C3%BChrung+in+METS+MODS</a></li><li>[6] Deutsche Digitale Bibliothek: IIIF. <a href='https://pro.deutsche-digitale-bibliothek.de/daten-nutzen/schnittstellen/international-image-interoperability-framework-iiif' target='_blank'>https://pro.deutsche-digitale-bibliothek.de/daten-nutzen/schnittstellen/international-image-interoperability-framework-iiif</a></li><li>[7] DFG-Viewer. <a href='https://dfg-viewer.de/' target='_blank'>https://dfg-viewer.de/</a></li><li>[8] Forschungsdaten.org: OAI-PMH. <a href='https://www.forschungsdaten.org/index.php/OAI-PMH' target='_blank'>https://www.forschungsdaten.org/index.php/OAI-PMH</a></li><li>[9] Wikipedia: Open Archives Initiative. <a href='https://de.wikipedia.org/wiki/Open_Archives_Initiative' target='_blank'>https://de.wikipedia.org/wiki/Open_Archives_Initiative</a></li></ul></p>"
    },
    {
      "@id": "portfolio:aufgabe-2",
      "@type": "schema:Assignment",
      "schema:name": "Aufgabe 2 - Linked Data",
      "schema:courseCode": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:linked-data"},
        {"@id": "portfolio:the-next-web"},
        {"@id": "portfolio:the-semantic-web"}
      ],
      "schema:content": "<h3>1. Bitte lesen Sie den Text: <a href='https://www.w3.org/DesignIssues/LinkedData.html' target='_blank'><i>Linked Data</i></a>. Fassen sie mit ihren eigenen Worten zusammen was Linked Data ausmacht.</h3><br><p>Linked Data beschreibt das Konzept zur Vernetzung von Daten im Web, die sowohl von Menschen als auch von Maschinen lesbar sind und die es ermöglichen, durch gezielt gesetzte Links Informationen zu entdecken. Es gibt vier Aspekte, die Linked Data ausmachen (nach Tim Berners-Lee): Alle Objekte werden durch einen eindeutigen Namen (URI) gekennzeichnet; Es werden HTTP-URIs verwendet, damit Menschen und Maschinen diese Objekte im Web aufrufen können. Beim Aufruf werden die Informationen hinter dem URI aufgerufen (Dereferenzierung); Bei einem Aufruf sollen zusätzliche Informationen durch RDF oder SPARQL bereitgestellt werden; Die Objekte beinhalten Links zu anderen verwandten Objekten. Durch das Verbinden der Objekte nach RDF entsteht ein durchsuchbarer Graph, in dem man verschiedene (auch unerwartete) Objekte wiederfinden kann, die Verbindungen zum gesuchten Objekt haben. Tim Berners-Lee ergänzt das Konzept um ein 5-Sterne-Schema für Linked (Open) Data: offene Lizenz, maschinenlesbar, offene Datenformate, RDF-Standards und Verlinkung. Wichtig: Linked Data kann intern und privat genutzt werden, aber es braucht eine offene Lizenz, damit es zu Linked Open Data wird.</p><br><h3>2. Sehen Sie sich den TED Talk von Tim Berners-Lee <a href='https://www.ted.com/talks/tim_berners_lee_the_next_web' target='_blank'><i>The next Web</i></a> an. Fassen mit ihren eigenen Worten zusammen, was das kommende Web ausmacht.</h3><br><p>Das nächste Web ist das Web der Daten, auch „Semantic Web“ genannt. Es geht darum, dass jeder Mensch, ob im beruflichen Umfeld, in der Forschung oder in der Regierung, seine Daten ins Web stellen kann und sollte. Doch es reicht nicht, Daten blind hochzuladen, sie würden sonst nur „rumliegen“, ohne Mehrwert zu schaffen. Entscheidend sind die Verbindungen zwischen den Daten. Die zugrundeliegende Technologie heißt „Linked Data“. Dabei werden standardisierte URIs und das Resource Description Framework (RDF) genutzt, um maschinenlesbare Verknüpfungen herzustellen. Im Gegensatz zu Google, das nur Dokumente indexiert, ermöglicht das Semantic Web semantische Anfragen. Man kann komplexe Fragen stellen und erhält präzise Antworten, etwas, das mit herkömmlichen Suchmaschinen bisher nicht möglich war. So entsteht kein Web der Dokumente oder nur der Daten, sondern ein Web, das Beziehungen und Bedeutung (Semantik) zwischen Daten abbildet. Dadurch können auch bisher ungestellte Fragen beantwortet werden. Jeder kann Daten einbringen, etwa über Social Media oder Initiativen wie OpenStreetMap. Doch entscheidend ist, dass Nutzer die Kontrolle über ihre Daten behalten, statt sie an Plattformen abzugeben. Ziel ist ein offenes, dezentrales Web, das alles verbindet, was Daten darstellen kann.</p><br><h3>3. Bitte lesen Sie den Text <a href='http://web.archive.org/web/20070206145045/http://www.ryerson.ca:80/~dgrimsha/courses/cps720_02/resources/Scientific%20American%20The%20Semantic%20Web.htm' target='_blank'><i>The Semantic Web</i></a> von Tim Berners-Lee, James Hendler und Ora Lassila durch. Fassen mit ihren eigenen Worten zusammen, was die Vision des Semantic Web ausmacht.</h3><br><p>Diese Aufgabe wollte ich zwar aus Interesse am Thema auch bearbeiten, aber der Aufruf der Webseite ergab immer wieder die folgende Fehlermeldung: <i>503 Service Unavailable. No server is available to handle this request.</i></p>"
    },
    {
      "@id": "portfolio:fazit",
      "@type": "schema:Event",
      "schema:name": "Fazit",
      "schema:startDate": "2026-01-26",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:vorlesung-1"},
        {"@id": "portfolio:vorlesung-2"},
        {"@id": "portfolio:vorlesung-3"},
        {"@id": "portfolio:vorlesung-4"},
        {"@id": "portfolio:vorlesung-5"},
        {"@id": "portfolio:aufgabe-1"},
        {"@id": "portfolio:aufgabe-2"}
      ],
      "schema:content": "<p>Abschließend kann ich durch aus behaupten, dass das Teilmodul <i>Datenformate</i>, auch wenn es nur im ersten Semester stattfindet, sich durch das ganze Semester zieht. Wieso? Weil man als Bibliotheksinformatiker jeden Tag mit verschiedenen Datenformaten in Berührung kommt. Sei es in Form in <i>ISBD</i> und <i>MARC</i>, oder <i>XML</i> und <i>JSON</i>. Vor allem in dem jetzigen Zeitalter ist es ein Wunder, wenn man damit nicht in Kontakt kommt. Allein im Studium merke ich das sehr. Wir haben noch andere Module im Studium, und in fast allen kommt das Thema nebei vor, bzw. wird vorausgesetzt. In meinem KI-Projekt, wo ich ein Linked-Data Datensatz erstelle für ein RAG-System, kommt Linked Data im YAML Format vor. Im Apple Vision Pro Projekt beschäftigen wir uns mit der Anreicherung von einer Medium durch Daten von Schnittstellen und verwenden hierbei auch SPARQL. Selbst bei diesem Lernportfolio kommen Datenformate und Metadaten vor. Ich muss zugeben, dass ein Lernportfolio in dieser Form nicht notwendig war, aber auf diese Weise konnte ich das JSON-LD Datenformat besser kennenlernen und mir allgemein ein besseres Bild von Linked Data machen. Und bei Internetprogrammierung ist es nur eine Frage der Zeit, bis wir und auch mit Datenformaten beschäftigen. In meinem privaten Leben beschäftige ich mich fast täglich mit Datenformaten und Linked Data, wenn ich meine Notizen von der Hochschule erstelle oder bearbeite, oder auch von anderen Dingen aus meinem Leben. Also ja, ich habe was gelernt, aber was und wie, auf das gehe ich nochmal in den folgenden Fragen ein:</p><br><p><b>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</b><br>Gelernt habe ich nicht nur, welche Datenformate existiere und wie sie aussehen, sondern auch die Unterscheidung zwischen denen und zwischen Schemasprachen und Strukturierungssprachen. Wichtige Begriffe wurden dabei geklärt und auch wenn das meiste für mich nichts neues war, da ich es bereits im Bachelorstudium hatte, so war es eine sehr gute Auffrischung der ganzen Thematik. Vor allem die theoretische Tiefe hat mir viel gebracht, da dies bei mir früher nur angeschnitten wurde.</p><br><p><b>Was war Ihr größter Lerngewinn?</b><br>Mein größter Lerngewinn war definitiv das Thema Linked Data. Nicht nur wurde mir endlich klar wie die Praxis bei Linked Data aussieht, sondern auch, wie man Ontologien erstellt. Wichtig für mich ist auch die persönliche Anwendung in meinem Obsidian Vault, da ich es nun, da ich das Wissen habe, auf das nächste Level bringen kann. Linked Data ist für mich ein Thema, welches sich bis Ende vom Studium durchziehen wird und ich kann mir sehr gut vorstellen in dieser Richtung eine Masterarbeit anzufertigen. Wie genau diese aber aussehen wird, wird sich noch zeigen.</p><br><p><b>Was war besonders überraschend? Was hat Sie irritiert?</b><br>Meine größte Überraschung war tatsächlich, wie viel sich über die ganzen theoretischen Dinge wie Datenmodelle, diskutieren lässt. Ich dachte, mehr oder weniger, dass man das beschließt und fertig ist. Es ist so definiert, also akzeptieren wir es alle. Aber nein! Auch Menschen die Tag für Tag in diesem Bereich arbeiten, diskutieren immer noch über verschiedene Definitionen und Strukturen (Siehe Vorlesung 1). Das hat mir gezeigt, dass das alles in Bewegung ist. Das finde ich toll! Ich hatte nie wirklich einen Einblick in die Praxis bei Datenformaten und Linked Data bekommen, und jetzt wo ich es hatte, kann ich viel besser behaupten, dass es mir gefällt, da es lebt.</p><br><p><b>Wo sehen Sie für sich noch weiteren Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</b><br>Was die Weiterbildung angeht, so sehe ich bei mir persönlich vor allem zwei Bereiche, die ich weiter vertiefen will: Einersteis natürlich Linked Data (Ich glaube das wurde aus dem gesamten Lernportfolio ziemlich deutlich). Ich finde diese Thematik äußerst interessant und will mein Wissen und mein Können in diesem Bereich weiter vertiefen und verfeinern. Das beinhaltet nicht nur theoretisches Wissen, in dem ich Bücher und Artikel darüber lesen will, sondern auch die praktische Übung durch die Einbindung von Linked Data in meine Hochschulprojekte als auch meine Privatprojekte. Anderseits wie bereit bei Vorlesung 4 erwähnt will ich mich unbedingt in SPARQL weiterbilden. Ich sehe sehr viel Potenzial darin, auch wenn ich es mir noch nicht 100%ig vorstellen kann, einfach weil mir das SPARQL Wissen fehlt. Die Syntax ist für mich schwer verständlich. Durch die Verwendung von SPARQL in einem unserem Proijek will ich das ändern und auch mich privat darin weiterbilden, sobald die Zeit verfügbar ist.</p>"
    },
    {
      "@id": "portfolio:patryk",
      "@type": "schema:Person",
      "schema:name": "Patryk Gadziomski",
      "schema:affiliation": {
        "@id": "portfolio:th-wildau"
      },
      "schema:memberOf": {
        "@id": "portfolio:studiengang"
      }
    },
    {
      "@id": "portfolio:tracy-arndt",
      "@type": "schema:Person",
      "schema:name": "Tracy Arndt",
      "schema:jobTitle": "Dozentin",
      "schema:affiliation": {
        "@id": "portfolio:th-wildau"
      }
    },
    {
      "@id": "portfolio:th-wildau",
      "@type": "schema:EducationalOrganization",
      "schema:name": "Technische Hochschule Wildau",
      "hasPart": {
        "@id": "portfolio:wit"
      }
    },
    {
      "@id": "portfolio:wit",
      "@type": "schema:EducationalOrganization",
      "schema:name": "Wildau Institue of Technology",
      "schema:isPartOf": [
        {"@id": "portfolio:th-wildau"}
      ]
    },
    {
      "@id": "portfolio:studiengang",
      "@type": "schema:EducationalOccupationalProgram",
      "schema:name": "Bibliotheksinformatik",
      "schema:provider": [
        {"@id": "portfolio:th-wildau"},
        {"@id": "portfolio:wit"}
      ],
      "hasPart": {
        "@id": "portfolio:modul"
      }
    },
    {
      "@id": "portfolio:modul",
      "@type": "schema:Course",
      "schema:name": "Schnittstellen und Datenformate",
      "schema:isPartOf": [
        {"@id": "portfolio:studiengang"}
      ],
      "schema:teacher": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:student": {
        "@id": "portfolio:patryk"
      }
    },
    {
      "@id": "portfolio:datenformate-gbv",
      "@type": "schema:CreativeWork",
      "schema:name": "Datenformate GBV",
      "schema:url": "https://format.gbv.de/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:handbuch-it-in-bibliotheken",
      "@type": "schema:CreativeWork",
      "schema:name": "Handbuch IT in Bibliotheken",
      "schema:url": "https://it-in-bibliotheken.de/contributors.html",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:understanding-metadata",
      "@type": "schema:CreativeWork",
      "schema:name": "Understanding Metadata: What is Metadata, and What is it For?",
      "schema:url": "https://www.niso.org/publications/understanding-metadata-2017",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:kleines-handbuch-metadaten",
      "@type": "schema:CreativeWork",
      "schema:name": "Kleines Handbuch Metadaten",
      "schema:url": "https://www.yumpu.com/de/document/view/23832049/kleines-handbuch-metadaten",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:zugang-gestalten",
      "@type": "schema:CreativeWork",
      "schema:name": "Zugang gestalten - Eine Anleitung für schlechte Standards",
      "schema:url": "https://www.youtube.com/watch?v=o51FOLsh4Ec",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:date-meme",
      "@type": "schema:CreativeWork",
      "schema:name": "Date Meme",
      "schema:url": "https://github.com/SamAmco/track-and-graph/issues/197#issuecomment-1445226139",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:einführung-in-skos",
      "@type": "schema:CreativeWork",
      "schema:name": "Einführung in SKOS am Beispiel von Open Educational Resources (OER)",
      "schema:url": "https://dini-ag-kim.github.io/skos-einfuehrung/#/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:marc-for-bibliographic",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc 21 Format for bibliographic data",
      "schema:url": "https://www.loc.gov/marc/bibliographic/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-dublin-core",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc to Dublin Core Crosswalk",
      "schema:url": "https://www.loc.gov/marc/marc2dc.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-21",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc 21",
      "schema:url": "https://www.dnb.de/DE/Professionell/Metadatendienste/Exportformate/MARC21/marc21_node.html",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:einführung-pica",
      "@type": "schema:CreativeWork",
      "schema:name": "EInführung in die Verarbeitung von PICA-Daten",
      "schema:url": "https://pro4bib.github.io/pica/#/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:processing-marc-21",
      "@type": "schema:CreativeWork",
      "schema:name": "Processing MARC 21",
      "schema:url": "https://jorol.github.io/processing-marc/#/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-must-die",
      "@type": "schema:CreativeWork",
      "schema:name": "MARC Must Die",
      "schema:url": "https://www.libraryjournal.com/story/marc-must-die",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-history-implications",
      "@type": "schema:CreativeWork",
      "schema:name": "MARC - Its history and implicaions",
      "schema:url": "https://scispace.com/pdf/marc-its-history-and-implications-3q74lbh06u.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:library-reference-model",
      "@type": "schema:CreativeWork",
      "schema:name": "IFLA Library Reference Model - A Conceptual Model for Bibliographic Information",
      "schema:url": "https://www.ifla.org/wp-content/uploads/2019/05/assets/cataloguing/frbr-lrm/ifla-lrm-august-2017.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:link-data-in-libraries",
      "@type": "schema:CreativeWork",
      "schema:name": "Linked Data in Libraries - A Case Study of Harvesting and Sharing Bibliographic Metadata with BIBFRAME",
      "schema:url": "https://www.researchgate.net/publication/276102000_Linked_Data_in_Libraries_A_Case_Study_of_Harvesting_and_Sharing_Bibliographic_Metadata_with_BIBFRAME",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:semantic-web",
      "@type": "schema:CreativeWork",
      "schema:name": "Semantic Web - Grundlagen",
      "schema:url": "https://link.springer.com/book/10.1007/978-3-540-33994-6",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:w3c-standards-and-drafts",
      "@type": "schema:CreativeWork",
      "schema:name": "W3C standards and drafts",
      "schema:url": "https://www.w3.org/TR/?tags[0]=data",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:semantic-web-working-ontologist",
      "@type": "schema:CreativeWork",
      "schema:name": "Semantic Web for the Working Ontologist - Effective Modeling in RDFS and OWL",
      "schema:url": "https://www.sciencedirect.com/book/monograph/9780123859655/semantic-web-for-the-working-ontologist",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:validating-rdf-data",
      "@type": "schema:CreativeWork",
      "schema:name": "Validating RDF Data",
      "schema:url": "https://book.validatingrdf.com/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-overview",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL 1.1 Overview",
      "schema:url": "https://www.w3.org/TR/sparql11-overview/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikidata-sparql-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "Wikidata: SPARQL Tutorial",
      "schema:url": "https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikidata-sparql-query-service",
      "@type": "schema:CreativeWork",
      "schema:name": "Wikidata:SPARQL query service/queries/examples",
      "schema:url": "https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-by-example",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL By Example: The Cheat Sheet",
      "schema:url": "https://www.iro.umontreal.ca/~lapalme/ift6281/sparql-1_1-cheat-sheet.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikibooks-sparql",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL",
      "schema:url": "https://en.wikibooks.org/wiki/SPARQL",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL Tutorial",
      "schema:url": "https://jena.apache.org/tutorials/sparql.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:learning-sparql",
      "@type": "schema:CreativeWork",
      "schema:name": "Learning SPARQL: Querying and Updating with SPARQL",
      "schema:url": "https://www.oreilly.com/library/view/learning-sparql-2nd/9781449371449/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:openrefine-user-manual",
      "@type": "schema:CreativeWork",
      "schema:name": "OpenRefine user manual",
      "schema:url": "https://openrefine.org/docs",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:metafacture-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "Metafacture Tutorial",
      "schema:url": "https://metafacture.github.io/metafacture-tutorial/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:linked-data",
      "@type": "schema:CreativeWork",
      "schema:name": "Linked Data",
      "schema:url": "https://www.w3.org/DesignIssues/LinkedData.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:the-next-web",
      "@type": "schema:CreativeWork",
      "schema:name": "The next Web",
      "schema:url": "https://www.ted.com/talks/tim_berners_lee_the_next_web",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:the-semantic-web",
      "@type": "schema:CreativeWork",
      "schema:name": "The Semantic Web",
      "schema:url": "http://web.archive.org/web/20070206145045/http://www.ryerson.ca:80/~dgrimsha/courses/cps720_02/resources/Scientific%20American%20The%20Semantic%20Web.htm",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:lernportfolio-prüfungsformat",
      "@type": "schema:CreativeWork",
      "schema:name": "Lernportfolio, Lerntagebuch und Peer Review als kompetenzorientierte und diversitätsgerechte Prüfungsformate",
      "schema:url": "https://www.fachportal-paedagogik.de/literatur/vollanzeige.html?FId=3208887",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:workflow-digital-sammlungen",
      "@type": "schema:CreativeWork",
      "schema:name": "Workflow der digitalisierten Sammlungen",
      "schema:url": "https://digital.staatsbibliothek-berlin.de/ueber-digitalisierte-sammlungen/digiworkflow",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:dalle-prompt",
      "@type": "schema:CreativeWork",
      "schema:name": "DALL-E Prompt für Vorlesung 2",
      "schema:url": "https://chatgpt.com/",
      "schema:inLanguage": "de",
      "schema:content": "Bitte erstelle mir anhand meiner Beschreibung das folgende Bild. Ich benötige das Bild für einen akademischen Artikel. Das Bild soll im Stil einer Skizze angefertigt werden. Das Bild soll im Querformat erstellt werden. Es gibt drei Schichten von unten nach oben. Schicht 1 zeigt das Semantic Web. Es zeigt Daten im Web, die über Links zu Linked Data werden und somit mit anderen Daten verbunden sind. Daraus ergibt sich ein riesiges Netzwerk an Daten. Schicht 2 stellt KI-Agenten dar, Computer, die Zugriff auf das Semantic Web haben und Anfragen der Benutzer auf diese Weise viel besser beantworten können. Die oberste Schicht stellt die Benutzer dar, die eine Anfrage an Schicht 2 stellen und genau die Informationen erhalten, die sie benötigen – und das mit einer semantischen Anfrage."
    }
  ]
}
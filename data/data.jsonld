{
  "@context": {
    "schema": "https://schema.org/",
    "portfolio": "https://patrykgadziomski.github.io/lcs-portfolio.github.io#",
    "relatedTo": "schema:relatedLink",
    "hasPart": "schema:hasPart"
  },
  "@graph": [
    {
      "@id": "portfolio:intro",
      "@type": "schema:Article",
      "schema:name": "Intro",
      "schema:startDate": "2026-01-16",
      "schema:dateModified": "2026-01-26",
      "schema:authors": {
        "@id": "portfolio:patryk"
      },
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:lernportfolio-prüfungsformat"}
      ],
      "schema:content": "<h2>Wilkommen auf meinem Lernportfolio!</h2><p>Dieses Portfolio dient der Dokumentation meiner Lernreise im Teilmodul Datenformate im Studiengang Bibliotheksinformatik an der Technischen Hochschule Wildau. Hier sammle ich Notizen und Erkenntnisse zu verschiedenen Themen aus dem Teilmodul.</p><br><p>Aber was ist ein Lernportfolio?<br><i>'Das Lernportfolio bezeichnet eine Sammlung von Dokumenten (z.B. Arbeitsblätter, Protokolle, Essays) oder Materialien [...], die zu einem bestimmten Zeitpunkt unter Bezug auf ein inhaltlich umrissenes Lehrgebiet die Leistungen [...] dokumentieren und charakterisieren.'</i> <a href='https://www.fachportal-paedagogik.de/literatur/vollanzeige.html?FId=3208887' target='_blank' class='custom-link'>[1]</a></p><br><p>Das Besondere an diesem Lernportfolio ist die Integration eines Knowledge Graphs auf der rechten Seite. Der Graph visualisiert die Struktur und Beziehungen zwischen den verschiedenen Notizen, ähnlich wie in Obsidian. Keine Werbung! Aber eine sehr tolle Software ;)</p><br><p><a href='https://obsidian.md/' target='_blank' class='custom-link'>--> Obsidian</a></p><br><p>Jeder Knoten repräsentiert eine Notiz, und die Verbindungen zeigen, wie die Themen miteinander zusammenhängen. Klicke auf einen Knoten im Graph, um direkt zum entsprechenden Artikel zu springen!</p><br><p>Der Aufbau dieses Lernportfolios ist in zeitliche Abschnitte aufgeteilt, je nach den Terminen, an denen die Vorlesungen stattfanden. In diesen wird kurz zusammengefasst was an diesen Tagen stattfand und was ich gelernt habe, in Bezug auf die Themen und andere Erkenntnisse.</p><br><p>Bei der technischen Umsetzung dieses Lernportfolios wurden Claude (ANTHROPIC) und Euria (INFOMANIAK) eingesetzt um die grundlegenden Funktionalitäten zu generieren. Weitere Funktionen, Strukturierung und Datenkuratierung wurde durch den Entwickler - Mich :) - durchgeführt. Der Text in diesem Portfolio wurde ohne jegliche Hilfe der Sprachmodelle erstellt.</p><br><p>Folgende Fragen werden am Ende als zusammenfassendes Fazit beantwortet:</p><br><ul style='padding-left: 15px;'><li>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</il><li>Was war ihr größter Lerngewinn?</li><li>Was war besonders überraschend? Was hat Sie irritiert?</li><li>Wo sehen Sie für sich noch Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</li></ul>"
    },
    {
      "@id": "portfolio:vorlesung-1",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 1",
      "schema:startDate": "2025-10-15",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:references": [
        {"@id": "portfolio:datenformate-gbv"},
        {"@id": "portfolio:handbuch-it-in-bibliotheken"},
        {"@id": "portfolio:understanding-metadata"},
        {"@id": "portfolio:kleines-handbuch-metadaten"},
        {"@id": "portfolio:zugang-gestalten"},
        {"@id": "portfolio:date-meme"},
        {"@id": "portfolio:einführung-in-skos"},
        {"@id": "portfolio:marc-for-bibliographic"},
        {"@id": "portfolio:marc-dublin-core"},
        {"@id": "portfolio:marc-21"},
        {"@id": "portfolio:einführung-pica"}
      ],
      "schema:content": "<p>Dies war der dritte Tag des Studiums der Bibliotheksinformatik an der TH Wildau, zugleich der erste Tag des Moduls <i>Schnittstellen und Datenformate</i>. Das Modul ist in zwei Teile aufgeteilt, wobei der erste Teil Datenformate behandelt, die von Tracy Arndt gelehrt werden.</p><br><p>Die Vorlesung begann mit den üblichen organisatorischen Inhalten und der Vorstellung. Anschließend wurden die Ziele des Teilmoduls sowie die einzelnen Blöcke vorgestellt. Der erste Block (Tag 1 und 2) befasst sich mit <i>Terminologie</i>, <i>Heterogenität</i>, <i>Interoperabilität</i> und <i>Organisation</i> von Daten. Natürlich durften auch die Prüfungsleistungen und die dazugehörigen Bewertungsdimensionen nicht fehlen. Das Lernportfolio als Prüfungsleistung finde ich persönlich sehr gut. Bereits im Bachelorstudium hatte ich oft diese Form der Prüfungsleistung. Das Schöne daran ist für mich die Freiheit, die man den Studierenden lässt, ihr Portfolio auf ihre ganz eigene Art zu erstellen. Außerdem bin ich der festen Überzeugung, dass man nur durch Reflexion des Gelernten merkt, ob man die Inhalte eines Moduls/eines Fachs verstanden hat. Manchmal führe ich selbst ein Lerntagebuch für Themen, die ich mir selbst beibringe, da ich mir dabei die Frage stellen muss, was ich überhaupt gelernt habe.</p><br><p>Zunächst klärten wir, was Daten überhaupt sind, also die <i>Terminologie</i>. Das war für mich nichts Neues, denn ich hatte in meinem Bachelor einen Schwerpunkt auf Daten gesetzt. Was Metadaten sind, war mir auch dadurch bereits klar (Metadatenmanagement wurde im Bachelorstudium belegt. Damals ausgiebig mit dem Datenformat <i>NeXus</i> im Rahmen einer Hausarbeit befasst). Bibliothekarische Metadaten waren mir jedoch nur zum Teil ein Begriff. Ich hatte zwar PICA und MARC im Studium, aber weder BibFrame noch Linked-Data-Formate. Linked Data finde ich äußerst interessant und möchte es überall dort verwenden, wo es möglich ist, um es wirklich zu verstehen und zu erfahren, wie es funktioniert. Wie sich zeigt, bekomme ich dazu auch die Gelegenheit (siehe Tag 4). Was das Thema <i>Heterogenität</i> angeht: Das war ein Thema, welches erst in der Arbeitspraxis wirklich sichtbar wurde für mich. Im Studium wurde immer gesagt, dass man Standards einhalten muss etc., doch eine richtige Rolle haben sie erst gespielt, als man dazu in Kontakt kam.</p><br><p>Im Anschluss daran folgte eine Gruppenarbeit, in der wir uns mit verschiedenen Begriffen auseinandersetzen sollten. Dazu wurden uns Quellen zur Verfügung gestellt. Meine Gruppe und ich entwarfen daraufhin eine Visualisierung. Die Ergebnisse wurden anschließend präsentiert. Es wurde rege darüber diskutiert, was genau das Datenmodell ist und ob bzw. wie überall Standards auftreten können. Es war interessant zu sehen, dass selbst Menschen, die täglich mit dieser Thematik arbeiten, Probleme damit haben, alles genau zu beschreiben. Das war für mich zum Teil ein <i>Aha</i>- und ein <i>Wow</i>-Moment. Früher dachte ich immer, dass man, sobald man in einem Bereich arbeitet, diesen komplett überblickt und versteht. Mittlerweile weiß ich es besser und auch, dass es eine ständige Lernreise ist. In der Gruppe haben wir viel diskutiert und die Abbildung komplett neu angefertigt.</p><br><img src='imgs/datenformate.svg' style='width: 100%;'><figcaption><b>Abbildung 1</b>: Gruppenarbeitsergebnis im Teilmodul Datenformate - Visualisierung zur Beschreibung verschiedener Daten-Begriffe und derer Zusammenhänge</figcaption><br><p>Spannend fand ich die <i>Interoperabilität</i> von Daten. Damit habe ich leider nicht viele Berührungspunkte, aber das möchte ich gerne ändern. Ich fand die Konvertierung zwischen Formaten schon immer spannend. Ein Skript zu schreiben, das ein Format in ein anderes konvertieren kann, hat etwas Beruhigendes. Dabei merke ich, dass ich statt vorhandener Software lieber eigene Skripte schreibe, was Vor- und Nachteile hat. Als <i>Kenntnis von Metadaten-Standards [..]</i> auf der Folie auftauchte, erinnerte ich mich an ein Gespräch mit einer alten Bekannten über Datenspeicherung von physikalischen Vorgängen (Flugfahrttechnik) und ihr Problem, dass sie nicht wusste, wie sie solche Daten speichern und diese dann am besten ins System weitergeben solle. Da hat das Wissen über Datenformate gefehlt, bzw. über spezielle Datenformate für physikalische Vorgänge/Experimente. Was man nicht weiß/kennt, kann man eben nicht verwenden.</p><br><p>Vor allem die Tatsache, dass es so viele Standards gibt und man sich auf keinen einigen kann (aus mehreren Gründen), fand ich sehr spannend. Im Modul <i>Künstliche Intelligenz</i> bin ich dabei, einen Linked-Data-Datensatz aufzubauen (dazu später mehr) und ich hätte fast einen eigenen Standard entwickelt, weil ich dachte: <i>Ah, wieso nicht? Dann kann ich es genau an meine Wünsche anpassen!</i> Am Ende habe ich mich doch für Standards entschieden, da sie alles beinhalteten, was ich brauchte; ich musste sie mir nur genauer anschauen.</p><br><p>In meinem Alltag kann ich durchaus behaupten, dass ich viel mit Daten zu tun habe. Das sollte jeder behaupten können, da wir von Daten umgeben sind. Im beruflichen Kontext habe ich nur mit ISBD und MARC zu tun, da ich in meiner Bibliothek die Katalogisierung mit Koha durchführe. In meiner anderen Tätigkeit bin ich mit Webentwicklung beschäftigt, weshalb ich auch mit Datenformaten umgehen können muss. Das alles war mir bereits aus dem Bachelorstudium bekannt. Dieser erste Tag war für mich somit eine nette Auffrischung, aber ich habe nicht viel Neues gelernt. Mein Hunger auf Linked Data wurde aber größer :)</p>"
    },
    {
      "@id": "portfolio:vorlesung-2",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 2",
      "schema:startDate": "2025-10-17",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-1"
      },
      "schema:references": [
        {"@id": "portfolio:einführung-pica"},
        {"@id": "portfolio:processing-marc-21"},
        {"@id": "portfolio:marc-must-die"},
        {"@id": "portfolio:marc-history-implications"},
        {"@id": "portfolio:handbuch-it-in-bibliotheken"},
        {"@id": "portfolio:library-reference-model"},
        {"@id": "portfolio:link-data-in-libraries"},
        {"@id": "portfolio:semantic-web"},
        {"@id": "portfolio:dalle-prompt"}
      ],
      "schema:content": "<p>An diesem Tag gingen wir tiefer in die Materie der Metadaten ein und begannen mit der Geschichte von <i>PICA</i> und <i>MARC</i>. Das Thema fand ich persönlich nicht besonders interessant, aber es war dennoch spannend zu sehen, wann diese entstanden sind und wie sich die Anforderungen an Metadatenformate gewandelt haben. Ich merke, dass mein Interesse eher der Verwendung der Daten bzw. Datenformate gilt als ihrer Geschichte.</p><p>Das größte Thema dieses Tages war die <i>Metadatenorganisation</i>. Ein hochspannendes Thema, das, finde ich, sehr kompliziert werden kann. Allein die Tatsache, dass es Austauschformate geben muss, da es so viele unterschiedliche Metadatenformate gibt, macht das Thema kompliziert. Ich habe mich gefragt, wieso es nicht ein Datenformat/Metadatenformat für alles geben kann. Aber das ist klar: Man kann nie alles abbilden. Selbst wenn man ein solches Format immer weiter erweitern würde, würde es so viele Felder beinhalten, dass es allein aus Speichergründen nicht benutzt werden könnte. Hier geht es vielmehr um die aufgabenspezifische Benutzung. Es ist zwar eine schöne Vorstellung, ein <i>Omni</i>-Datenformat/Metadatenformat zu entwickeln, aber auch eine unmögliche.</p><br><p>Was man hierbei wunderbar sehen kann: Metadaten gibt es überall. Egal, in welchem Bereich man arbeitet, kommt man immer mit ihnen in Kontakt, auch wenn man es vielleicht nicht weiß. Für mich wurde auch noch einmal klar, wo überall Metadaten vorkommen. Überall! In jedem Datenfluss kommen Daten und Metadaten vor. Ich habe mir in diesem Zusammenhang meine alten Arbeiten aus dem Bachelorstudium noch einmal angeschaut und gemerkt, an wie vielen Stellen das eigentlich der Fall war, ich aber nicht so sehr darauf eingegangen bin (oder gar nicht). Jetzt hätte ich nur gerne die Zeit, alles noch einmal durchzugehen und abzuändern.</p><br><p>Das FRBR-Modell war mir bereits aus dem Bachelor bekannt. Ich wusste jedoch nicht, dass es mehrere Varianten gibt bzw. dass es weiterentwickelt wurde. Das FRBR-LRM hat mich sehr überrascht. Da es sich aber um ein sehr theoretisches Modell handelt, entsteht für mich persönlich wieder das gleiche Problem, dass sich damit nicht alles abbilden lässt und man sich ewig darüber streiten kann. Beim Thema <i>ETL</i> muss ich wieder gestehen, dass es mir bekannt war, da ich im Bachelorstudium auch viele Module aus dem Bereich <i>Data Science</i> belegt hatte.</p><br><p>Danach kam das unglaublich tolle Thema <i>Linked (Open) Data</i>. Ich hatte im Rahmen meines Studiums bereits damit zu tun, da sich mein Professor damit beschäftigte, aber ich konnte mir darunter nie etwas vorstellen. Für mich war es wieder eine sehr theoretische Vorstellung. Das Web, in dem Daten als Knoten definiert sind, die wiederum Verbindungen zu anderen Knoten und somit zu anderen Daten darstellen. Theoretisch war es mir klar, aber ich konnte mir beim besten Willen nicht vorstellen, wie es in der Praxis benutzt werden sollte (ohne zu wissen, dass ich es selbst seit Jahren benutze...). Aber endlich habe ich es verstanden! Es ist ein Knowledge Graph! Ein Knowledge Graph entsteht also durch das Verbinden von Daten (Nodes). Das ist dann Linked Data! Ich finde die Idee, dass das Web im Web-3.0-Modell komplett aus Daten besteht, die aufeinander verweisen und somit verbunden sind, unglaublich schön. Allein die Möglichkeit, semantische Anfragen zu stellen, um genau die Daten zu finden, die man braucht, ist für die Suche und somit für Suchmaschinen enorm wichtig. Leider wurde das durch das Aufkommen der großen Sprachmodelle verhindert. bzw. gebremst. Ich stelle mir ein perfektes Modell vielmehr als eine Verbindung von beiden Dingen vor. Das Semantic Web könnte eine unglaubliche Daten- und Informationsquelle darstellen, auf die KI-Agenten zugreifen könnten. Ich habe dazu einmal eine Abbildung gesehen, finde diese aber leider nicht mehr. Ich habe es mit ChatGPTs DALL-E versucht zu visualisieren.</p><br><img src='imgs/web3andAI.png' style='width: 100%;'><figcaption><b>Abbildung 2</b>: Vorstellung eines Web-Models, bei welchem Semantic Web und KI-Agenten harmonisch koexistieren - erstellt mit ChatGPTs DALL-E</figcaption><br><p>Jetzt: Warum denke ich, dass ich Linked Data bereits seit Jahren benutze?</p><br><p>Ich benutze seit Jahren <i>Obsidian</i>, um meine Notizen zu erstellen und zu verwalten. Die Software erlaubt es, Verbindungen zwischen Notizen zu erstellen. Somit war mir dieses Prinzip, Informationen zu verbinden, bereits sehr gut bekannt. Aber endlich habe ich es vollständig verstanden und die einzelnen Knoten in meinem Kopf zum Thema Linked Data haben sich endlich <i>verbunden</i> :) Ich habe meine Notizen die ganze Zeit wie Linked Data behandelt, ohne es zu wissen. Seitdem benutze ich Standards wie Schema, Dublin Core und Friend of a Friend, um meine Daten noch besser zu strukturieren und zu verknüpfen. Wieso? Im Modul <i>Künstliche Intelligenz</i> bearbeite ich mit einer Kommilitonin ein Projekt zum Thema <i>Retrieval Augmented Generation</i>. Dafür erstelle ich, wie bereits gesagt, einen Linked-Data-Datensatz. Die Idee ist es, am Ende ein RAG-System zu haben, mit dem man im Obsidian Vault suchen bzw. dem RAG-System Fragen stellen kann. Dabei soll das RAG-System nicht nur die Similarity der Anfrage und der Dokumente in der Vektor-Datenbank beachten, sondern auch die Dokumente, welche mit dem Dokumenten verbunden sind. Sehr interessant war auch die Tatsache, dass Linked Data besser für das Training oder die Benutzung von KI-Modellen wie LLMs geeignet ist. Das passt sehr gut zu unserem Projekt. Wenn es für das Projekt gut funktioniert, werde ich es für meinen privaten Vault nutzen und kann somit mit <i>mir selbst reden</i>, da dieser viele Informationen aus meinem Leben beinhaltet. (Der Datenschutz muss allerdings noch beachtet werden, denn ich möchte nicht, dass meine Daten am Ende in irgendein LLM geworfen werden.)</p>"
    },
    {
      "@id": "portfolio:vorlesung-3",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 3",
      "schema:startDate": "2025-12-03",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-2"
      },
      "schema:references": [
        {"@id": "portfolio:semantic-web"},
        {"@id": "portfolio:w3c-standards-and-drafts"},
        {"@id": "portfolio:semantic-web-working-ontologist"},
        {"@id": "portfolio:validating-rdf-data"}
      ],
      "schema:content": "<p>Das Thema <i>Linked Data</i> bzw. <i>Semantic Web</i> wurde an diesem Tag weiter vertieft. Und man kann durchaus behaupten, dass meine Lernkurve in diesem Bereich exponentiell war.</p><br><p>An diesem Tag habe ich endlich verstanden, wie man <i>Ontologien</i> aufbaut und <i>RDF-Standards</i> benutzt. (Zwar habe ich damit zuvor bereits etwas experimentiert, doch durch die Übung, die wir während der Vorlesung gemacht haben, wurde alles viel klarer.) Bisher dachte ich immer, dass Ontologien sehr komplexe, textuelle Bäume sind. Das trifft so gesehen zu, aber auch wieder nicht.</p><br><p>PS: Vor dem Master habe ich mir Jobs im Bereich der Ontologien angeschaut und dachte, dass sie zwar interessant klingen, ich es aber bestimmt nicht hinbekomme, weil mir das Wissen fehlt. Jetzt weiß ich: Mir hat das Wissen gefehlt, aber die Praxis war da. Ich wusste nur nicht, dass das, was ich in Obsidian erstellt habe, mehr oder weniger, Ontologien waren.</p><br><p>Ontologien lassen sich als das darstellen, was ich sowieso fast jeden Tag gemacht habe bzw. für mein privates Leben viel tue. Man erstellt Beschreibungen von Objekten und deren Beziehungen zueinander in Form eines Knowledge Graphs. Das war wie eine Erleuchtung für mich. Anhand dieser Ontologien kann man Objekte beschreiben. Dafür gibt es verschiedene Standards. Das war ein größer Lerneffekt und ein noch größerer <i>Aha</i>-Moment. Vor allem durch das Üben mit <i>WebVOWL</i> wurde mir klar, wie Ontologien aussehen können.</p><br><p>Zwei Übungen wurden durchgeführt, welche mir unglaublich viel gebrach haben: <a href='data/PG_Ontology.ttl' target='_blank'>Eigene Ontologie</a> und <a href='data/bim25graph.ttl' target='_blank'>BIM25 Ontologie</a>.</p><br><p>Neben diesem Thema wurde <i>SPARQL</i> kurz angesprochen, das aber am nächsten Tag genauer behandelt wird. Das RDF-Schema als Tripel kannte ich tatsächlich bereits. Es war somit leicht verständlich und man konnte schnell in die Übungen einsteigen. Was ich nicht kannte, waren die Standrds wie <i>Schema</i>, <i>FoaF</i>, oder <i>Dublin Core</i> als auch die <i>RDF-Serialisierung</i>. Es war jedoch alles sehr verständlich.</p><br><p>Für das Lernportfolio habe ich mich für JOSN-LD entschieden, als mein Datenformat für Linked Data. Der Grund war die einfache Lesbarkeit, da es auf der Syntax von JSON basiert und ein modernes Datenformat ist. Am Anfang finden Definitionen im Context statt, unter anderem auch von meinem Standard, da ich für das Lernportfolio teilweise sehr spezifische Beziehungen gebraucht habe, bzw. es einfach mal probieren wollte, wie es mit einem eigenen funktionieren würde. Ich hätte hier natürlich Standards benutzen können wie <i>Schema</i>, was bereits verwendet wird, aber der Lerneffekt war somit größer. Der weitere Aufbau des Datensatzes erfolgte in der Form, wie man eine JSON Datei schreibt, mit der Ausnahme, das die Keys die folgende Sytanx beinhalten: <i>standard:relation</i>. Die Erstellung dieses Datensatzes sah ich als eine sehr wichtige Übung für mich, die Arbeit mit einem tatsächlichen Linked Data Datenformat zu üben. Vor allem auch die Visualisierung als Knowledge Graph.</p><br><p>Sehr schöne Ressourcen die genannt wurden will ich hier nochmal verewigen: <ul style='padding-left: 15px;'><li><a href='https://www.ldf.fi/service/rdf-grapher' target='_blank'>RDF Grapher</a></li><li><a href='https://issemantic.net/rdf-visualizer' target='_blank'>Visualize RDF graph linked data as a connected diagram</a></li><li><a href='https://issemantic.net/rdf-converter' target='_blank'>Online RDF converter and validator to JSON-LD, Microdata, Turtle, TriG, RDF-star or any other serialization format</a></li><li><a href='http://ttl.summerofcode.be/' target='_blank'>IDLab Turtle Validator</a></li></ul></p<br><br><p>Und jetzt nochmal zu Thema Jobsuche: Jetzt da ich endlich verstanden habe was Linked Data und Ontologien sind, dachte ich, ich schau wieder nach Jobs in diesem Bereich. Das hier soll eine kleine Sammlung sein, die mich später daran erinnern soll, was in diesem Bereich möglich ist. (Die Links könnten mittlerweile ungültig sein, da sie Stellenanzeige abgelaufen ist)</p><br><ul style='padding-left: 15px;'><li><b>Wissenschaftliche:r Mitarbeiter:in (w/d/m) Schwerpunkt Wissensgraph</b> - <a href='https://karriere.preussischer-kulturbesitz.de/jobposting/e13514c77a524fa3a13a6f4d3c4b2ab2b69c8f710' target='_blank' class='custom-link'>Link</a> - Dabei geht es um den Aufbau und Pflege eines Wissensgraphen sowie der Datenmodellierung und Ontologieintegration. Klingt mittlerweile nicht mehr so abstrakt für mich :)</li><li><b>Wissenschaftliche:r Mitarbeiter:in (w/d/m) Schwerpunkt ETL-Prozesse & Ontologie-Entwicklung</b> - <a href='https://karriere.preussischer-kulturbesitz.de/jobposting/4443429e02664d89ef4df65a1d610bcd884c7abe0' target='_blank' class='custom-link'>Link</a> - Hier lese ich viel über Transformationsmodellen mit Datenformate, die mir leider noch nicht ganz bekannt vorkommen, aber auch der Erarbeitung von Ontologien!</li><li><b>Knowledge Engineer (m/w/d)</b> - <a href='https://de.indeed.com/viewjob?jk=245f60421081e29d&tk=1jf98dn8chn1h85b&from=serp&vjs=3' target='_blank' class='custom-link'>Link</a> - Diese Stelle, auch wenn Worte wie Ontologien, Metadaten und Wissensgraphen gefallen sind, ist viel wirtschaftlicher und vielvältiger aufgestellt. Hier muss man auch Data Science Erfahrung haben</li><li><b>Senior Developer (f/m/d) for Knowledge Graph Research</b> - <a href='https://jobtensor.com/job/Senior-Developer-fmd-for-Knowledge-Graph-Research-8518c0254b1d' target='_blank' class='custom-link'>Link</a> - Eine sehr interessante Stelle, mit einer riesigen akademischen Verantwortung. Nichts für mich (noch nicht).</li><li><b>Knowledge and Ontology Engineer - Insurance (m/f/d)*</b> - <a href='https://jobtensor.com/job/Knowledge-and-Ontology-Engineer-Insurance-mfd-340bfa3ff9d6' target='_blank' class='custom-link'>Link</a> - Wieder eine sehr wirtschaftliche Stelle, die auch wieder sehr im Bereich von Data Science liegt.</b></i></ul><br><p>Was mit bei der Recherche aufgefallen ist, ist, dass viele Stelle sehr verwandt zu den Stellen als Data Engineer oder Data Scientist sind. Mir ist klar, dass Man Ontologien und Knowledge Graphs auch in diesen Bereichen einsetzt, aber hier muss man stark achten, zwischen bibliothekarischen Stellen und wirtschaftlichen Stellen. Ich fand es sehr schön die Stellenanzeigen lesen zu können, ohne das gleiche Gefühl wie damals zu verspüren, da ich jetzt das Wissen aus dem Teilmodul habe. Jetzt muss ich es nur in der Praxis umsetzen und meine Fähigkeiten in diesem Bereich weiter ausbauen.</p>"
    },
    {
      "@id": "portfolio:vorlesung-4",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 4",
      "schema:startDate": "2025-12-04",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-3"
      },
      "schema:references": [
        {"@id": "portfolio:sparql-overview"},
        {"@id": "portfolio:wikidata-sparql-tutorial"},
        {"@id": "portfolio:wikidata-sparql-query-service"},
        {"@id": "portfolio:sparql-by-example"},
        {"@id": "portfolio:wikibooks-sparql"},
        {"@id": "portfolio:sparql-tutorial"},
        {"@id": "portfolio:learning-sparql"}
      ],
      "schema:content": "<p>An diesem Tag wurde <i>SPARQL</i> vorgestellt. Ich muss zugeben, dass es der Tag war, an dem ich am wenigsten mitgenommen habe. Nicht, weil das Thema nicht spannend ist. Ich finde es unglaublich, was man mit SPARQL alles abfragen kann. Aber ich bin einfach nicht für Online-Vorlesungen gemacht. In einer normalen Vorlesung höre ich zu, achte auf und stelle gelegentlich Fragen. Dabei kann ich mich komplett darauf einstellen. Bei einer Online-Vorlesung funktioniert das nicht, meine Konzentration schweift komplett ab. Aus diesem Grund fiel es mir unglaublich schwer, der Vorlesung zu folgen, aber ein paar Dinge konnte ich doch mitnehmen.</p><br><p>Mit SPARQL hatte ich bisher nur einen Berührungspunkt: Im Bachelor-Modul <i>Metadatenmanagement</i> habe ich es kennengelernt, aber ich muss gestehen, dass ich es bis heute nicht ganz durchblicke. Mir ist durchaus verständlich, wie SPARQL funktioniert, aber die Syntax ist mein größtes Problem. Ich müsste mir theoretisch alle Codes für die Entitäten merken, wenn es keine Liste gibt bzw. ich keine zur Hand habe. Das ist mein Problem. Das ist sehr ungewohnt. Wenn ich Python-Code oder eine SQL-Abfrage schreibe, muss ich normalerweise nichts nachschauen, außer wie die Spalten heißen. Ich weiß allerdings nicht, ob man das so vergleichen kann. Aus diesem Grund und aufgrund meiner mangelnden Online-Konzentration hatte ich Schwierigkeiten, den Übungsaufgaben zu folgen. Was mir tatsächlich leichtfiel, waren die Funktionen in SPARQL wie Filtern oder Gruppieren, da diese sehr ähnlich zu SQL sind. Ich habe mir aber vorgenommen, das nachzuholen, denn ich sehe ein riesiges Potenzial für Information Retrieval und eventuell auch für meine privaten Projekte, vor allem, da man auch geografische Daten abfragen kann. Hierbei finde ich auch die Visualisierungen der Ergebnisse der Abfragen sehr hilfreich. Nicht nur lassen sich einfache Graphen wie Balkendiagramme ausgeben, sondern ebenfalls Karten und Timelines. Im Modul <i>Management and Leadership</i>, in welchem wir ein Projekt mit der Apple Vision Pro durchführen, haben wir uns vorgenommen aus SPARQL zu nutzen. Das Ziel der Teilgruppe in der ich Mitglied bin, ist es zu schauen, wie man Medien in Bibliotheken, oder anderen Kultureinrichtungen, während man die Apple Vision Pro verwendet, anreichern kann. Da wir Informationen über ein bestimmtes Objekt bekommen müssen, bietet sich SPARQL besonders gut an.</p><br><p>Persönlich habe ich mir vorgenommen, das Buch <i>Learning SPARQL: Querying and Updating with SPARQL</i> von O’Reilly Media durchzuarbeiten, um am Ende behaupten zu können, SPARQL wirklich verstanden zu haben. Da ich aber derzeit kein Projekt habe, in dem ich SPARQL verwenden muss, und auch nicht die Zeit habe, das Buch durchzuarbeiten, wird sich das Thema SPARQL für mich leider noch etwas nach hinten verschieben. Ein anderes Thema für mich war noch Wikidata.org, welches mir ebenfalls schon bekannt war. Ich habe in Vergangenheit bei Wikipedia Workshops teilgenommen und fand diese ganze Initiative immer schon besonders spannend. Bei Wikipedia wurde ich jedoch nie ganz warm. Wikidata finde ich persönlich ein unglaubliches Werk, wo man sehr viele Informationen über alle möglichen Dinge erfährt und abfragen kann. SPARQL ist dabei ein Muss. Ein Grund mehr SPARQL zu lernen."
    },
    {
      "@id": "portfolio:vorlesung-5",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 5",
      "schema:startDate": "2026-01-16",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-4"
      },
      "schema:references": [
        {"@id": "portfolio:openrefine-user-manual"},
        {"@id": "portfolio:metafacture-tutorial"}
      ],
      "schema:content": "<p>Unverzichtbar sind Werkzeuge, mit denen sich Daten ansehen, bearbeiten, analysieren und verarbeiten lassen. Dabei wurden unterschiedliche Softwares gezeigt und zwei davon haben wir in einem kleinen Workshop selbst ausprobiert.</p><br><p>Am letzten Tag des Teilmoduls fand ein Workshop zu <i>OpenRefine</i> und <i>Metafacture</i> statt. Bei Open Refine wurden verschiedene Funktionen anhand der Dokumentation gezeigt und ausprobiert. Bei Metafacture wurden die Funktionen anhand eines Tutorials gezeigt. Persönlich nutze ich solche Software in meiner Praxis nicht wirklich. Wenn ich Operationen zum Filtern oder Clustering großer Datenmengen durchführe, dann mache ich das meistens in Python mit Bibliotheken, die mit großen Datenmengen arbeiten können. Da ich das oft tue, bin ich mit Python bei der Datenanalyse auch schneller als mit Software mit einer GUI. Ich kann aber nachvollziehen, dass eine GUI-Software für Personen, die nicht oft programmieren, von Vorteil ist. Sehr praktisch fand ich den Abgleich der Daten mit anderen Datensätzen (z. B. dem Datensatz der DNB).</p><br><p>Bei Metafacture war mein Eindruck ähnlich. Ich fand das Einlesen eines Datensatzes (z. B. in JSON) und das Exportieren in einem anderen Datenformat aber sehr praktisch. Ansonsten konnte ich für meine alltägliche Arbeit keinen großen Nutzen daraus ziehen. Das kann sich aber noch ändern.</p><br><p>Am Ende folgte eine kleine Diskussion, die mehr oder weniger genau in diese Richtung ging. Einerseits hat man das Potenzial der Software gesehen, aber da sie nur im Nischenbereich von sehr wenigen Leuten eingesetzt wird, ist es fragwürdig, wieso man sie überhaupt lernen sollte. Vor allem, da man sich erst mit der Software auseinandersetzen müsste, da sie nicht ganz unkomplex ist. Ich glaube, die in der Gruppe, die täglich programmieren, sind es eher gewohnt, solche Operationen mit Python und Pandas durchzuführen (solange der Speicher es erlaubt), während die anderen das Problem haben, dass sie in ihrer alltäglichen Arbeit einfach keine Verwendung für solche Daten-Software finden, da sie nicht täglich mit Daten arbeiten."
    },
    {
      "@id": "portfolio:aufgabe-1",
      "@type": "schema:Assignment",
      "schema:name": "Aufgabe 1 - Datenworkflow",
      "schema:courseCode": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:workflow-digital-sammlungen"}
      ],
      "schema:content": "<h3>Die Staatsbibliothek zu Berlin hat auf ihrer Webseite den <a href='https://digital.staatsbibliothek-berlin.de/ueber-digitalisierte-sammlungen/digiworkflow' target='_blank'>Workflow für die Digitalisierung</a> veröffentlicht.<br>Erstellen sie aus der Liste ein Diagramm das den Workflow grafisch darstellt. Reflektieren sie anschließend den Workflow und beschreiben sie die eingesetzten Datenformate und Schnittstellen (TIFF, METS/MODS, IIIF-Manifest, OAI-PMH)</h3><br><img src='imgs/digitalisierte_sammlungen.svg' style='width: 100%;'>"
    },
    {
      "@id": "portfolio:aufgabe-2",
      "@type": "schema:Assignment",
      "schema:name": "Aufgabe 2 - Linked Data",
      "schema:courseCode": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:linked-data"}
      ],
      "schema:content": "<h3>Bitte lesen Sie den Text: <a href='https://www.w3.org/DesignIssues/LinkedData.html' target='_blank'>Linked Data</a>. Fassen sie mit ihren eigenen Worten zusammen was Linked Data ausmacht.</h3><br><p>Linked Data beschreibt das Konzept zur Vernetzung von Daten im Web, die sowohl von Menschen als auch von Maschinen lesbar sind und die es ermöglichen, durch gezielt gesetzte Links Informationen zu entdecken.</p><br><p>Es gibt vier Aspekte, die Linked Data ausmachen (nach Tim Berners-Lee): Alle Objekte werden durch einen eindeutigen Namen (URI) gekennzeichnet; Es werden HTTP-URIs verwendet, damit Menschen und Maschinen diese Objekte im Web aufrufen können. Beim Aufruf werden die Informationen hinter dem URI aufgerufen (Dereferenzierung); Bei einem Aufruf sollen zusätzliche Informationen durch RDF oder SPARQL bereitgestellt werden; Die Objekte beinhalten Links zu anderen verwandten Objekten.</p><br><p>Durch das Verbinden der Objekte nach RDF entsteht ein durchsuchbarer Graph, in dem man verschiedene (auch unerwartete) Objekte wiederfinden kann, die Verbindungen zum gesuchten Objekt haben. Tim Berners-Lee ergänzt das Konzept um ein 5-Sterne-Schema für Linked (Open) Data: offene Lizenz, maschinenlesbar, offene Datenformate, RDF-Standards und Verlinkung. Wichtig: Linked Data kann intern und privat genutzt werden, aber es braucht eine offene Lizenz, damit es zu Linked Open Data wird.</p>"
    },
    {
      "@id": "portfolio:fazit",
      "@type": "schema:Event",
      "schema:name": "Fazit",
      "schema:startDate": "2026-01-26",
      "schema:dateModified": "2026-01-26",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:vorlesung-1"},
        {"@id": "portfolio:vorlesung-2"},
        {"@id": "portfolio:vorlesung-3"},
        {"@id": "portfolio:vorlesung-4"},
        {"@id": "portfolio:vorlesung-5"}
      ],
      "schema:content": "<p>Abschließend kann ich durch aus behaupten, dass das Teilmodul <i>Datenformate</i>, auch wenn es nur im ersten Semester stattfindet, sich durch das ganze Semester zieht. Wieso? Weil man als Bibliotheksinformatiker jeden Tag mit verschiedenen Datenformaten in Berührung kommt. Sei es in Form in <i>ISBD</i> und <i>MARC</i>, oder <i>XML</i> und <i>JSON</i>. Vor allem in dem jetzigen Zeitalter ist es ein Wunder, wenn man damit nicht in Kontakt kommt. Allein im Studium merke ich das sehr. Wir haben noch andere Module im Studium, und in fast allen kommt das Thema nebei vor, bzw. wird vorausgesetzt. In meinem KI-Projekt, wo ich ein Linked-Data Datensatz erstelle für ein RAG-System, kommt Linked Data im YAML Format vor. Im Apple Vision Pro Projekt beschäftigen wir uns mit der Anreicherung von einer Medium durch Daten von Schnittstellen und verwenden hierbei auch SPARQL. Selbst bei diesem Lernportfolio kommen Datenformate und Metadaten vor. Ich muss zugeben, dass ein Lernportfolio in dieser Form nicht notwendig war, aber auf diese Weise konnte ich das JSON-LD Datenformat besser kennenlernen und mir allgemein ein besseres Bild von Linked Data machen. Und bei Internetprogrammierung ist es nur eine Frage der Zeit, bis wir und auch mit Datenformaten beschäftigen. In meinem privaten Leben beschäftige ich mich fast täglich mit Datenformaten und Linked Data, wenn ich meine Notizen von der Hochschule erstelle oder bearbeite, oder auch von anderen Dingen aus meinem Leben. Also ja, ich habe was gelernt, aber was und wie, auf das gehe ich nochmal in den folgenden Fragen ein:</p><br><p><b>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</b><br>Gelernt habe ich nicht nur, welche Datenformate existiere und wie sie aussehen, sondern auch die Unterscheidung zwischen denen und zwischen Schemasprachen und Strukturierungssprachen. Wichtige Begriffe wurden dabei geklärt und auch wenn das meiste für mich nichts neues war, da ich es bereits im Bachelorstudium hatte, so war es eine sehr gute Auffrischung der ganzen Thematik. Vor allem die theoretische Tiefe hat mir viel gebracht, da dies bei mir früher nur angeschnitten wurde.</p><br><p><b>Was war Ihr größter Lerngewinn?</b><br>Mein größter Lerngewinn war definitiv das Thema Linked Data. Nicht nur wurde mir endlich klar wie die Praxis bei Linked Data aussieht, sondern auch, wie man Ontologien erstellt. Wichtig für mich ist auch die persönliche Anwendung in meinem Obsidian Vault, da ich es nun, da ich das Wissen habe, auf das nächste Level bringen kann. Linked Data ist für mich ein Thema, welches sich bis Ende vom Studium durchziehen wird und ich kann mir sehr gut vorstellen in dieser Richtung eine Masterarbeit anzufertigen. Wie genau diese aber aussehen wird, wird sich noch zeigen.</p><br><p><b>Was war besonders überraschend? Was hat Sie irritiert?</b><br>Meine größte Überraschung war tatsächlich, wie viel sich über die ganzen theoretischen Dinge wie Datenmodelle, diskutieren lässt. Ich dachte, mehr oder weniger, dass man das beschließt und fertig ist. Es ist so definiert, also akzeptieren wir es alle. Aber nein! Auch Menschen die Tag für Tag in diesem Bereich arbeiten, diskutieren immer noch über verschiedene Definitionen und Strukturen (Siehe Vorlesung 1). Das hat mir gezeigt, dass das alles in Bewegung ist. Das finde ich toll! Ich hatte nie wirklich einen Einblick in die Praxis bei Datenformaten und Linked Data bekommen, und jetzt wo ich es hatte, kann ich viel besser behaupten, dass es mir gefällt, da es lebt.</p><br><p><b>Wo sehen Sie für sich noch weiteren Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</b><br>Was die Weiterbildung angeht, so sehe ich bei mir persönlich vor allem zwei Bereiche, die ich weiter vertiefen will: Einersteis natürlich Linked Data (Ich glaube das wurde aus dem gesamten Lernportfolio ziemlich deutlich). Ich finde diese Thematik äußerst interessant und will mein Wissen und mein Können in diesem Bereich weiter vertiefen und verfeinern. Das beinhaltet nicht nur theoretisches Wissen, in dem ich Bücher und Artikel darüber lesen will, sondern auch die praktische Übung durch die Einbindung von Linked Data in meine Hochschulprojekte als auch meine Privatprojekte. Anderseits wie bereit bei Vorlesung 4 erwähnt will ich mich unbedingt in SPARQL weiterbilden. Ich sehe sehr viel Potenzial darin, auch wenn ich es mir noch nicht 100%ig vorstellen kann, einfach weil mir das SPARQL Wissen fehlt. Die Syntax ist für mich schwer verständlich. Durch die Verwendung von SPARQL in einem unserem Proijek will ich das ändern und auch mich privat darin weiterbilden, sobald die Zeit verfügbar ist.</p>"
    },
    {
      "@id": "portfolio:patryk",
      "@type": "schema:Person",
      "schema:name": "Patryk Gadziomski",
      "schema:affiliation": {
        "@id": "portfolio:th-wildau"
      },
      "schema:memberOf": {
        "@id": "portfolio:studiengang"
      }
    },
    {
      "@id": "portfolio:tracy-arndt",
      "@type": "schema:Person",
      "schema:name": "Tracy Arndt",
      "schema:jobTitle": "Dozentin",
      "schema:affiliation": {
        "@id": "portfolio:th-wildau"
      }
    },
    {
      "@id": "portfolio:th-wildau",
      "@type": "schema:EducationalOrganization",
      "schema:name": "Technische Hochschule Wildau",
      "hasPart": {
        "@id": "portfolio:wit"
      }
    },
    {
      "@id": "portfolio:wit",
      "@type": "schema:EducationalOrganization",
      "schema:name": "Wildau Institue of Technology",
      "schema:isPartOf": [
        {"@id": "portfolio:th-wildau"}
      ]
    },
    {
      "@id": "portfolio:studiengang",
      "@type": "schema:EducationalOccupationalProgram",
      "schema:name": "Bibliotheksinformatik",
      "schema:provider": [
        {"@id": "portfolio:th-wildau"},
        {"@id": "portfolio:wit"}
      ],
      "hasPart": {
        "@id": "portfolio:modul"
      }
    },
    {
      "@id": "portfolio:modul",
      "@type": "schema:Course",
      "schema:name": "Schnittstellen und Datenformate",
      "schema:isPartOf": [
        {"@id": "portfolio:studiengang"}
      ],
      "schema:teacher": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:student": {
        "@id": "portfolio:patryk"
      }
    },
    {
      "@id": "portfolio:datenformate-gbv",
      "@type": "schema:CreativeWork",
      "schema:name": "Datenformate GBV",
      "schema:url": "https://format.gbv.de/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:handbuch-it-in-bibliotheken",
      "@type": "schema:CreativeWork",
      "schema:name": "Handbuch IT in Bibliotheken",
      "schema:url": "https://it-in-bibliotheken.de/contributors.html",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:understanding-metadata",
      "@type": "schema:CreativeWork",
      "schema:name": "Understanding Metadata: What is Metadata, and What is it For?",
      "schema:url": "https://www.niso.org/publications/understanding-metadata-2017",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:kleines-handbuch-metadaten",
      "@type": "schema:CreativeWork",
      "schema:name": "Kleines Handbuch Metadaten",
      "schema:url": "https://www.yumpu.com/de/document/view/23832049/kleines-handbuch-metadaten",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:zugang-gestalten",
      "@type": "schema:CreativeWork",
      "schema:name": "Zugang gestalten - Eine Anleitung für schlechte Standards",
      "schema:url": "https://www.youtube.com/watch?v=o51FOLsh4Ec",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:date-meme",
      "@type": "schema:CreativeWork",
      "schema:name": "Date Meme",
      "schema:url": "https://github.com/SamAmco/track-and-graph/issues/197#issuecomment-1445226139",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:einführung-in-skos",
      "@type": "schema:CreativeWork",
      "schema:name": "Einführung in SKOS am Beispiel von Open Educational Resources (OER)",
      "schema:url": "https://dini-ag-kim.github.io/skos-einfuehrung/#/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:marc-for-bibliographic",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc 21 Format for bibliographic data",
      "schema:url": "https://www.loc.gov/marc/bibliographic/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-dublin-core",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc to Dublin Core Crosswalk",
      "schema:url": "https://www.loc.gov/marc/marc2dc.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-21",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc 21",
      "schema:url": "https://www.dnb.de/DE/Professionell/Metadatendienste/Exportformate/MARC21/marc21_node.html",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:einführung-pica",
      "@type": "schema:CreativeWork",
      "schema:name": "EInführung in die Verarbeitung von PICA-Daten",
      "schema:url": "https://pro4bib.github.io/pica/#/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:processing-marc-21",
      "@type": "schema:CreativeWork",
      "schema:name": "Processing MARC 21",
      "schema:url": "https://jorol.github.io/processing-marc/#/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-must-die",
      "@type": "schema:CreativeWork",
      "schema:name": "MARC Must Die",
      "schema:url": "https://www.libraryjournal.com/story/marc-must-die",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-history-implications",
      "@type": "schema:CreativeWork",
      "schema:name": "MARC - Its history and implicaions",
      "schema:url": "https://scispace.com/pdf/marc-its-history-and-implications-3q74lbh06u.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:library-reference-model",
      "@type": "schema:CreativeWork",
      "schema:name": "IFLA Library Reference Model - A Conceptual Model for Bibliographic Information",
      "schema:url": "https://www.ifla.org/wp-content/uploads/2019/05/assets/cataloguing/frbr-lrm/ifla-lrm-august-2017.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:link-data-in-libraries",
      "@type": "schema:CreativeWork",
      "schema:name": "Linked Data in Libraries - A Case Study of Harvesting and Sharing Bibliographic Metadata with BIBFRAME",
      "schema:url": "https://www.researchgate.net/publication/276102000_Linked_Data_in_Libraries_A_Case_Study_of_Harvesting_and_Sharing_Bibliographic_Metadata_with_BIBFRAME",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:semantic-web",
      "@type": "schema:CreativeWork",
      "schema:name": "Semantic Web - Grundlagen",
      "schema:url": "https://link.springer.com/book/10.1007/978-3-540-33994-6",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:w3c-standards-and-drafts",
      "@type": "schema:CreativeWork",
      "schema:name": "W3C standards and drafts",
      "schema:url": "https://www.w3.org/TR/?tags[0]=data",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:semantic-web-working-ontologist",
      "@type": "schema:CreativeWork",
      "schema:name": "Semantic Web for the Working Ontologist - Effective Modeling in RDFS and OWL",
      "schema:url": "https://www.sciencedirect.com/book/monograph/9780123859655/semantic-web-for-the-working-ontologist",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:validating-rdf-data",
      "@type": "schema:CreativeWork",
      "schema:name": "Validating RDF Data",
      "schema:url": "https://book.validatingrdf.com/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-overview",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL 1.1 Overview",
      "schema:url": "https://www.w3.org/TR/sparql11-overview/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikidata-sparql-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "Wikidata: SPARQL Tutorial",
      "schema:url": "https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikidata-sparql-query-service",
      "@type": "schema:CreativeWork",
      "schema:name": "Wikidata:SPARQL query service/queries/examples",
      "schema:url": "https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-by-example",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL By Example: The Cheat Sheet",
      "schema:url": "https://www.iro.umontreal.ca/~lapalme/ift6281/sparql-1_1-cheat-sheet.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikibooks-sparql",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL",
      "schema:url": "https://en.wikibooks.org/wiki/SPARQL",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL Tutorial",
      "schema:url": "https://jena.apache.org/tutorials/sparql.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:learning-sparql",
      "@type": "schema:CreativeWork",
      "schema:name": "Learning SPARQL: Querying and Updating with SPARQL",
      "schema:url": "https://www.oreilly.com/library/view/learning-sparql-2nd/9781449371449/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:openrefine-user-manual",
      "@type": "schema:CreativeWork",
      "schema:name": "OpenRefine user manual",
      "schema:url": "https://openrefine.org/docs",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:metafacture-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "Metafacture Tutorial",
      "schema:url": "https://metafacture.github.io/metafacture-tutorial/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:linked-data",
      "@type": "schema:CreativeWork",
      "schema:name": "Linked Data",
      "schema:url": "https://www.w3.org/DesignIssues/LinkedData.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:lernportfolio-prüfungsformat",
      "@type": "schema:CreativeWork",
      "schema:name": "Lernportfolio, Lerntagebuch und Peer Review als kompetenzorientierte und diversitätsgerechte Prüfungsformate",
      "schema:url": "https://www.fachportal-paedagogik.de/literatur/vollanzeige.html?FId=3208887",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:workflow-digital-sammlungen",
      "@type": "schema:CreativeWork",
      "schema:name": "Workflow der digitalisierten Sammlungen",
      "schema:url": "https://digital.staatsbibliothek-berlin.de/ueber-digitalisierte-sammlungen/digiworkflow",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:dalle-prompt",
      "@type": "schema:CreativeWork",
      "schema:name": "DALL-E Prompt für Vorlesung 2",
      "schema:url": "https://chatgpt.com/",
      "schema:inLanguage": "de",
      "schema:content": "Bitte erstelle mir anhand meiner Beschreibung das folgende Bild. Ich benötige das Bild für einen akademischen Artikel. Das Bild soll im Stil einer Skizze angefertigt werden. Das Bild soll im Querformat erstellt werden. Es gibt drei Schichten von unten nach oben. Schicht 1 zeigt das Semantic Web. Es zeigt Daten im Web, die über Links zu Linked Data werden und somit mit anderen Daten verbunden sind. Daraus ergibt sich ein riesiges Netzwerk an Daten. Schicht 2 stellt KI-Agenten dar, Computer, die Zugriff auf das Semantic Web haben und Anfragen der Benutzer auf diese Weise viel besser beantworten können. Die oberste Schicht stellt die Benutzer dar, die eine Anfrage an Schicht 2 stellen und genau die Informationen erhalten, die sie benötigen – und das mit einer semantischen Anfrage."
    }
  ]
}